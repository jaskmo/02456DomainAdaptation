{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physionet data in memory\n",
      "Hospital data in memory\n",
      "Mnist\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from flip_gradient import flip_gradient\n",
    "from utils import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_names = ['W','N1','N2','N3','R']\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "num_subjects_physionet = 20\n",
    "num_subjects_hospital = 17\n",
    "#data_physionet = pickle.load(open('C:/Users/ramuol/Documents/dataOut.pkl','rb'))\n",
    "#data_hospital = pickle.load(open('C:/Users/ramuol/Documents/hDataOut.pkl', 'rb'))\n",
    "data_physionet = pickle.load(open('/home/jaskmo/Documents/DataCollection/sleep-edfx/PickleJar/dataOut.pkl','rb'))\n",
    "data_hospital = pickle.load(open('/home/jaskmo/Documents/DataCollection/sleep-edfx/PickleJar/hDataOut.pkl','rb'))\n",
    "\n",
    "#Get random index for training and test for the physionet data (3 subjects are kept for test)\n",
    "random_perm_physionet = np.random.permutation(num_subjects_physionet)\n",
    "idx_tmp_physionet = random_perm_physionet[range(num_subjects_physionet - 3)]\n",
    "idx_test_physionet = random_perm_physionet[(num_subjects_physionet - 3) : num_subjects_physionet]\n",
    "\n",
    "#Get random index for training and test for the hospital data\n",
    "random_perm_hospital = np.random.permutation(num_subjects_hospital)\n",
    "idx_tmp_hospital = random_perm_hospital[range(num_subjects_hospital - 3)]\n",
    "idx_test_hospital = random_perm_hospital[(num_subjects_hospital- 3) : num_subjects_hospital]\n",
    "        \n",
    "#Get the datasets for the data from physionet\n",
    "inputs_train_phys, targets_train_phys, inputs_val_phys, targets_val_phys, inputs_test_phys, targets_test_phys = get_data_complete(\n",
    "   idx_tmp_physionet, idx_test_physionet, data_physionet, 'physionet')\n",
    "print(\"Physionet data in memory\")\n",
    "\n",
    "inputs_train_hosp, targets_train_hosp, inputs_val_hosp, targets_val_hosp, inputs_test_hosp, targets_test_hosp = get_data_complete(\n",
    "   idx_tmp_hospital, idx_test_hospital, data_hospital, 'hospital')\n",
    "print(\"Hospital data in memory\")\n",
    "\n",
    "# Create a mixed dataset for TSNE visualization\n",
    "num_test = 25\n",
    "combined_test_imgs = np.vstack([inputs_train_phys[:num_test], inputs_train_hosp[:num_test]])\n",
    "combined_test_labels = np.vstack([targets_train_phys[:num_test], targets_train_hosp[:num_test]])\n",
    "combined_test_domain = np.vstack([np.tile([1., 0.], [num_test, 1]),\n",
    "       np.tile([0., 1.], [num_test, 1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "beta = 0.001\n",
    "\n",
    "class MNISTModel():\n",
    "    \"\"\"Simple MNIST domain adaptation model.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \n",
    "        self.noClasses = 5\n",
    "        self.X = tf.placeholder(tf.uint8, [None, 224, 224, 3])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.noClasses])\n",
    "        self.domain = tf.placeholder(tf.float32, [None, 2])\n",
    "        self.l = tf.placeholder(tf.float32, [])\n",
    "        self.train = tf.placeholder(tf.bool, [])\n",
    "        \n",
    "        X_input = tf.cast(self.X, tf.float32)/255\n",
    "               \n",
    "        # CNN model for feature extraction\n",
    "        with tf.variable_scope('feature_extractor'):\n",
    "            \n",
    "            # First CNN\n",
    "            W_conv0 = weight_variable([3, 3, 3, 64])\n",
    "            b_conv0 = bias_variable([64])\n",
    "            h_conv0 = tf.nn.relu(conv2d(X_input, W_conv0) + b_conv0)\n",
    "           \n",
    "            W_conv1 = weight_variable([3, 3, 64, 64])\n",
    "            b_conv1 = bias_variable([64])\n",
    "            h_conv1 = tf.nn.relu(conv2d(h_conv0, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            \n",
    "            # Second CNN\n",
    "            W_conv2 = weight_variable([3, 3, 64, 128])\n",
    "            b_conv2 = bias_variable([128])\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "                       \n",
    "            W_conv3 = weight_variable([3, 3, 128, 128])\n",
    "            b_conv3 = bias_variable([128])\n",
    "            h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3) + b_conv3)\n",
    "            h_pool3 = max_pool_2x2(h_conv3)\n",
    "                                    \n",
    "            # Third CNN\n",
    "            W_conv4 = weight_variable([3, 3, 128, 256])\n",
    "            b_conv4 = bias_variable([256])\n",
    "            h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "                        \n",
    "            W_conv5 = weight_variable([3, 3, 256, 256])\n",
    "            b_conv5 = bias_variable([256])\n",
    "            h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5) + b_conv5)\n",
    "                             \n",
    "            W_conv6 = weight_variable([3, 3, 256, 256])\n",
    "            b_conv6 = bias_variable([256])\n",
    "            h_conv6 = tf.nn.relu(conv2d(h_conv5, W_conv6) + b_conv6)\n",
    "            h_pool6 = max_pool_2x2(h_conv6)\n",
    "            \n",
    "            # Fourth CNN\n",
    "            W_conv7 = weight_variable([3, 3, 256, 512])\n",
    "            b_conv7 = bias_variable([512])\n",
    "            h_conv7 = tf.nn.relu(conv2d(h_pool6, W_conv7) + b_conv7)\n",
    "            \n",
    "            W_conv8 = weight_variable([3, 3, 512, 512])\n",
    "            b_conv8 = bias_variable([512])\n",
    "            h_conv8 = tf.nn.relu(conv2d(h_conv7, W_conv8) + b_conv8)\n",
    "                            \n",
    "            W_conv9 = weight_variable([3, 3, 512, 512])\n",
    "            b_conv9 = bias_variable([512])\n",
    "            h_conv9 = tf.nn.relu(conv2d(h_conv8, W_conv9) + b_conv9)\n",
    "            h_pool9 = max_pool_2x2(h_conv9)\n",
    "                        \n",
    "            # Fifth CNN\n",
    "            W_conv10 = weight_variable([3, 3, 512, 512])\n",
    "            b_conv10 = bias_variable([512])\n",
    "            h_conv10 = tf.nn.relu(conv2d(h_pool9, W_conv10) + b_conv10)\n",
    "                        \n",
    "            W_conv11 = weight_variable([3, 3, 512, 512])\n",
    "            b_conv11 = bias_variable([512])\n",
    "            h_conv11 = tf.nn.relu(conv2d(h_conv10, W_conv11) + b_conv11)\n",
    "                            \n",
    "            W_conv12 = weight_variable([3, 3, 512, 512])\n",
    "            b_conv12 = bias_variable([512])\n",
    "            h_conv12 = tf.nn.relu(conv2d(h_conv11, W_conv12) + b_conv12)\n",
    "            h_pool12 = max_pool_2x2(h_conv12)\n",
    "                        \n",
    "            # The domain-invariant feature\n",
    "            self.shape = int(np.prod(h_pool12.get_shape()[1:]))\n",
    "\n",
    "            self.feature = tf.reshape(h_pool12, [-1, self.shape])\n",
    "            \n",
    "        # MLP for class prediction\n",
    "        with tf.variable_scope('label_predictor'):\n",
    "            \n",
    "            # Switches to route target examples (second half of batch) differently\n",
    "            # depending on train or test mode.\n",
    "            all_features = lambda: self.feature\n",
    "            source_features = lambda: tf.slice(self.feature, [0, 0], [(int)(batch_size / 2), -1])\n",
    "            classify_feats = tf.cond(self.train, source_features, all_features)\n",
    "            \n",
    "            all_labels = lambda: self.y\n",
    "            source_labels = lambda: tf.slice(self.y, [0, 0], [int(batch_size / 2), -1])\n",
    "            self.classify_labels = tf.cond(self.train, source_labels, all_labels)\n",
    "            \n",
    "            W_fc0 = dense_variable([self.shape, 2048])\n",
    "            b_fc0 = bias_variable([2048])\n",
    "            h_fc0 = tf.layers.dropout(tf.nn.relu(tf.matmul(classify_feats, W_fc0) + b_fc0), rate=0.5)\n",
    "\n",
    "            W_fc1 = dense_variable([2048, 1024])\n",
    "            b_fc1 = bias_variable([1024])\n",
    "            h_fc1 = tf.layers.dropout(tf.nn.relu(tf.matmul(h_fc0, W_fc1) + b_fc1), rate=0.5)\n",
    "\n",
    "            W_fc2 = dense_variable([1024, self.noClasses])\n",
    "            b_fc2 = bias_variable([self.noClasses])\n",
    "            logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "            \n",
    "            self.pred = tf.nn.softmax(logits)\n",
    "            self.pred_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.classify_labels)\n",
    "\n",
    "        # Small MLP for domain prediction with adversarial loss\n",
    "        with tf.variable_scope('domain_predictor'):\n",
    "            \n",
    "            # Flip the gradient when backpropagating through this operation\n",
    "            feat = flip_gradient(self.feature, self.l)\n",
    "            d_W_fc0 = dense_variable([self.shape, 2048])\n",
    "            d_b_fc0 = bias_variable([2048])\n",
    "            d_h_fc0 = tf.layers.dropout(tf.nn.relu(tf.matmul(feat, d_W_fc0) + d_b_fc0), 0.5)\n",
    "            \n",
    "            d_W_fc1 = dense_variable([2048, 1024])\n",
    "            d_b_fc1 = bias_variable([1024])\n",
    "            d_h_fc1 = tf.layers.dropout(tf.nn.relu(tf.matmul(d_h_fc0, d_W_fc1) + d_b_fc1), rate=0.5)\n",
    "            \n",
    "            d_W_fc2 = dense_variable([1024, 2])\n",
    "            d_b_fc2 = bias_variable([2])\n",
    "            d_logits = tf.matmul(d_h_fc1, d_W_fc2) + d_b_fc2\n",
    "            \n",
    "            self.domain_pred = tf.nn.softmax(d_logits)\n",
    "            self.domain_loss = tf.nn.softmax_cross_entropy_with_logits(logits=d_logits, labels=self.domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function initializing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars          = tf.global_variables()\n",
    "    is_not_initialized   = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    vgg_vars = not_initialized_vars[:26]\n",
    "    other_vars = not_initialized_vars[26:]\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(other_vars))\n",
    "     \n",
    "    weight_file= '/home/jaskmo/Documents/DataCollection/sleep-edfx/PickleJar/vgg16_weights.npz'\n",
    "#     weight_file= 'D:/vgg16_weights.npz'\n",
    "\n",
    "    weights = np.load(weight_file)\n",
    "    keys = sorted(weights.keys())\n",
    "    for i, k in enumerate(keys):\n",
    "        if i < 26:\n",
    "            sess.run(vgg_vars[i].assign(weights[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model graph\n",
    "graph = tf.get_default_graph()\n",
    "with graph.as_default():\n",
    "    model = MNISTModel()\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, [])\n",
    "    \n",
    "    pred_loss = tf.reduce_mean(model.pred_loss)\n",
    "    domain_loss = tf.reduce_mean(model.domain_loss)\n",
    "    total_loss = pred_loss + domain_loss\n",
    "    regularize = tf.contrib.layers.l2_regularizer(beta)\n",
    "    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    reg_term = sum([regularize(param) for param in params])\n",
    "    total_loss += reg_term\n",
    "\n",
    "    regular_train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(pred_loss)\n",
    "    dann_train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_loss)\n",
    "    \n",
    "    # Evaluation\n",
    "    correct_label_pred = tf.equal(tf.argmax(model.classify_labels, 1), tf.argmax(model.pred, 1))\n",
    "    label_acc = tf.reduce_mean(tf.cast(correct_label_pred, tf.float32))\n",
    "    correct_domain_pred = tf.equal(tf.argmax(model.domain, 1), tf.argmax(model.domain_pred, 1))\n",
    "    domain_acc = tf.reduce_mean(tf.cast(correct_domain_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain adaptation training\n",
      "Epoch number:  1\n",
      "TRAINING:   Total loss: 4.979692, Domain acc: 0.660000, Label acc: 0.553333, Label loss: 1.062077, Domain adapt para: 0.000000  lr: 0.000100\n",
      "VALIDATION: Total loss: 3.603035, Domain acc: 0.659622, Label acc: 0.764948, Label loss: 0.637957 \n",
      "[[519   5   0   0   0]\n",
      " [330 804  46  43  33]\n",
      " [  2   7  46  11  18]\n",
      " [  2  60 102 339  71]\n",
      " [  8   5  15   1  53]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.60      0.99      0.75       524\n",
      "         N1       0.91      0.64      0.75      1256\n",
      "         N2       0.22      0.55      0.31        84\n",
      "         N3       0.86      0.59      0.70       574\n",
      "          R       0.30      0.65      0.41        82\n",
      "\n",
      "avg / total       0.79      0.70      0.71      2520\n",
      "\n",
      "0.69881\n",
      "Epoch number:  2\n",
      "TRAINING:   Total loss: 3.482853, Domain acc: 0.667667, Label acc: 0.703333, Label loss: 0.788579, Domain adapt para: 0.099668  lr: 0.000100\n",
      "VALIDATION: Total loss: 3.236132, Domain acc: 0.560138, Label acc: 0.748797, Label loss: 0.649335 \n",
      "[[522   2   0   0   0]\n",
      " [501 597  52  66  40]\n",
      " [  2   4  50  10  18]\n",
      " [  5  24  80 371  94]\n",
      " [ 12   3  13   1  53]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.50      1.00      0.67       524\n",
      "         N1       0.95      0.48      0.63      1256\n",
      "         N2       0.26      0.60      0.36        84\n",
      "         N3       0.83      0.65      0.73       574\n",
      "          R       0.26      0.65      0.37        82\n",
      "\n",
      "avg / total       0.78      0.63      0.64      2520\n",
      "\n",
      "0.632143\n",
      "Epoch number:  3\n",
      "TRAINING:   Total loss: 3.045515, Domain acc: 0.647667, Label acc: 0.746333, Label loss: 0.683014, Domain adapt para: 0.197375  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.842470, Domain acc: 0.673024, Label acc: 0.772165, Label loss: 0.594398 \n",
      "[[504  20   0   0   0]\n",
      " [160 966  54  53  23]\n",
      " [  1   7  61   0  15]\n",
      " [  0  35  74 384  81]\n",
      " [  8   6  17   0  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.75      0.96      0.84       524\n",
      "         N1       0.93      0.77      0.84      1256\n",
      "         N2       0.30      0.73      0.42        84\n",
      "         N3       0.88      0.67      0.76       574\n",
      "          R       0.30      0.62      0.40        82\n",
      "\n",
      "avg / total       0.84      0.78      0.80      2520\n",
      "\n",
      "0.780159\n",
      "Epoch number:  4\n",
      "TRAINING:   Total loss: 2.812712, Domain acc: 0.667333, Label acc: 0.740333, Label loss: 0.691084, Domain adapt para: 0.291313  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.768548, Domain acc: 0.569072, Label acc: 0.780069, Label loss: 0.590779 \n",
      "[[510  13   0   0   1]\n",
      " [240 861  71  58  26]\n",
      " [  0   8  48  21   7]\n",
      " [  0  41  84 394  55]\n",
      " [  7   3  17   5  50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.67      0.97      0.80       524\n",
      "         N1       0.93      0.69      0.79      1256\n",
      "         N2       0.22      0.57      0.32        84\n",
      "         N3       0.82      0.69      0.75       574\n",
      "          R       0.36      0.61      0.45        82\n",
      "\n",
      "avg / total       0.81      0.74      0.75      2520\n",
      "\n",
      "0.739286\n",
      "Epoch number:  5\n",
      "TRAINING:   Total loss: 2.638492, Domain acc: 0.649667, Label acc: 0.763000, Label loss: 0.645511, Domain adapt para: 0.379949  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.734385, Domain acc: 0.555842, Label acc: 0.770447, Label loss: 0.663779 \n",
      "[[499  20   0   0   5]\n",
      " [206 803 113  92  42]\n",
      " [  2   3  53   0  26]\n",
      " [  0   3 185 304  82]\n",
      " [  7   3  15   0  57]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.70      0.95      0.81       524\n",
      "         N1       0.97      0.64      0.77      1256\n",
      "         N2       0.14      0.63      0.24        84\n",
      "         N3       0.77      0.53      0.63       574\n",
      "          R       0.27      0.70      0.39        82\n",
      "\n",
      "avg / total       0.81      0.68      0.71      2520\n",
      "\n",
      "0.680952\n",
      "Epoch number:  6\n",
      "TRAINING:   Total loss: 2.546597, Domain acc: 0.626333, Label acc: 0.758333, Label loss: 0.653274, Domain adapt para: 0.462117  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.480838, Domain acc: 0.612028, Label acc: 0.764605, Label loss: 0.606903 \n",
      "[[512  11   0   0   1]\n",
      " [287 792 120  44  13]\n",
      " [  1   4  70   0   9]\n",
      " [  0  16 127 387  44]\n",
      " [ 11   2  27   0  42]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.63      0.98      0.77       524\n",
      "         N1       0.96      0.63      0.76      1256\n",
      "         N2       0.20      0.83      0.33        84\n",
      "         N3       0.90      0.67      0.77       574\n",
      "          R       0.39      0.51      0.44        82\n",
      "\n",
      "avg / total       0.83      0.72      0.74      2520\n",
      "\n",
      "0.715476\n",
      "Epoch number:  7\n",
      "TRAINING:   Total loss: 2.437189, Domain acc: 0.628167, Label acc: 0.766333, Label loss: 0.633191, Domain adapt para: 0.537050  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.374702, Domain acc: 0.587629, Label acc: 0.782818, Label loss: 0.589027 \n",
      "[[507  16   0   0   1]\n",
      " [236 834  89  66  31]\n",
      " [  0   4  68   2  10]\n",
      " [  0   8  98 427  41]\n",
      " [  7   2  28   0  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.68      0.97      0.80       524\n",
      "         N1       0.97      0.66      0.79      1256\n",
      "         N2       0.24      0.81      0.37        84\n",
      "         N3       0.86      0.74      0.80       574\n",
      "          R       0.35      0.55      0.43        82\n",
      "\n",
      "avg / total       0.84      0.75      0.77      2520\n",
      "\n",
      "0.746429\n",
      "Epoch number:  8\n",
      "TRAINING:   Total loss: 2.353560, Domain acc: 0.620167, Label acc: 0.774333, Label loss: 0.626642, Domain adapt para: 0.604368  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.473928, Domain acc: 0.499313, Label acc: 0.731959, Label loss: 0.685629 \n",
      "[[521   3   0   0   0]\n",
      " [432 684  72  53  15]\n",
      " [  2   3  63   1  15]\n",
      " [  2  33 105 382  52]\n",
      " [  9   2  18   1  52]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.54      0.99      0.70       524\n",
      "         N1       0.94      0.54      0.69      1256\n",
      "         N2       0.24      0.75      0.37        84\n",
      "         N3       0.87      0.67      0.76       574\n",
      "          R       0.39      0.63      0.48        82\n",
      "\n",
      "avg / total       0.80      0.68      0.69      2520\n",
      "\n",
      "0.675397\n",
      "Epoch number:  9\n",
      "TRAINING:   Total loss: 2.257027, Domain acc: 0.617333, Label acc: 0.775000, Label loss: 0.599627, Domain adapt para: 0.664037  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.339938, Domain acc: 0.547595, Label acc: 0.763574, Label loss: 0.660384 \n",
      "[[512  11   0   0   1]\n",
      " [291 775 101  52  37]\n",
      " [  0   4  69   0  11]\n",
      " [  0  34 141 341  58]\n",
      " [  6   2  26   0  48]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.63      0.98      0.77       524\n",
      "         N1       0.94      0.62      0.74      1256\n",
      "         N2       0.20      0.82      0.33        84\n",
      "         N3       0.87      0.59      0.71       574\n",
      "          R       0.31      0.59      0.41        82\n",
      "\n",
      "avg / total       0.81      0.69      0.72      2520\n",
      "\n",
      "0.69246\n",
      "Epoch number:  10\n",
      "TRAINING:   Total loss: 2.207915, Domain acc: 0.600833, Label acc: 0.779333, Label loss: 0.602910, Domain adapt para: 0.716298  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.346159, Domain acc: 0.483505, Label acc: 0.733677, Label loss: 0.703423 \n",
      "[[504  14   1   4   1]\n",
      " [248 686 148 165   9]\n",
      " [  1   2  69   3   9]\n",
      " [  0   0 105 450  19]\n",
      " [  8   2  34   0  38]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.66      0.96      0.78       524\n",
      "         N1       0.97      0.55      0.70      1256\n",
      "         N2       0.19      0.82      0.31        84\n",
      "         N3       0.72      0.78      0.75       574\n",
      "          R       0.50      0.46      0.48        82\n",
      "\n",
      "avg / total       0.81      0.69      0.71      2520\n",
      "\n",
      "0.693254\n",
      "Epoch number:  11\n",
      "TRAINING:   Total loss: 2.152141, Domain acc: 0.596667, Label acc: 0.779333, Label loss: 0.595299, Domain adapt para: 0.761594  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.190662, Domain acc: 0.520447, Label acc: 0.789003, Label loss: 0.596989 \n",
      "[[491  21   1   0  11]\n",
      " [213 744 135 101  63]\n",
      " [  0   3  62   3  16]\n",
      " [  0   3 199 284  88]\n",
      " [  3   3  20   0  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.69      0.94      0.80       524\n",
      "         N1       0.96      0.59      0.73      1256\n",
      "         N2       0.15      0.74      0.25        84\n",
      "         N3       0.73      0.49      0.59       574\n",
      "          R       0.24      0.68      0.35        82\n",
      "\n",
      "avg / total       0.80      0.65      0.69      2520\n",
      "\n",
      "0.649603\n",
      "Epoch number:  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING:   Total loss: 2.106925, Domain acc: 0.595333, Label acc: 0.787667, Label loss: 0.603851, Domain adapt para: 0.800499  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.108137, Domain acc: 0.604296, Label acc: 0.771134, Label loss: 0.618890 \n",
      "[[499  24   0   0   1]\n",
      " [174 897 107  62  16]\n",
      " [  0   4  47   0  33]\n",
      " [  0  13 147 378  36]\n",
      " [  6   3  17   0  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.73      0.95      0.83       524\n",
      "         N1       0.95      0.71      0.82      1256\n",
      "         N2       0.15      0.56      0.23        84\n",
      "         N3       0.86      0.66      0.75       574\n",
      "          R       0.39      0.68      0.50        82\n",
      "\n",
      "avg / total       0.84      0.74      0.77      2520\n",
      "\n",
      "0.744841\n",
      "Epoch number:  13\n",
      "TRAINING:   Total loss: 2.057323, Domain acc: 0.578667, Label acc: 0.785333, Label loss: 0.590964, Domain adapt para: 0.833655  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.016692, Domain acc: 0.630241, Label acc: 0.782818, Label loss: 0.575956 \n",
      "[[498  24   0   0   2]\n",
      " [244 818  67 114  13]\n",
      " [  1   4  62  12   5]\n",
      " [  0   4  61 477  32]\n",
      " [  8   3  27   1  43]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.66      0.95      0.78       524\n",
      "         N1       0.96      0.65      0.78      1256\n",
      "         N2       0.29      0.74      0.41        84\n",
      "         N3       0.79      0.83      0.81       574\n",
      "          R       0.45      0.52      0.49        82\n",
      "\n",
      "avg / total       0.82      0.75      0.76      2520\n",
      "\n",
      "0.753175\n",
      "Epoch number:  14\n",
      "TRAINING:   Total loss: 2.006503, Domain acc: 0.602167, Label acc: 0.786667, Label loss: 0.592549, Domain adapt para: 0.861723  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.133319, Domain acc: 0.484021, Label acc: 0.752234, Label loss: 0.674839 \n",
      "[[501  21   0   0   2]\n",
      " [236 756 130 109  25]\n",
      " [  2   2  58   1  21]\n",
      " [  0   2 170 382  20]\n",
      " [  8   2  24   0  48]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.67      0.96      0.79       524\n",
      "         N1       0.97      0.60      0.74      1256\n",
      "         N2       0.15      0.69      0.25        84\n",
      "         N3       0.78      0.67      0.72       574\n",
      "          R       0.41      0.59      0.48        82\n",
      "\n",
      "avg / total       0.82      0.69      0.72      2520\n",
      "\n",
      "0.69246\n",
      "Epoch number:  15\n",
      "TRAINING:   Total loss: 1.945620, Domain acc: 0.604167, Label acc: 0.789667, Label loss: 0.568377, Domain adapt para: 0.885352  lr: 0.000100\n",
      "VALIDATION: Total loss: 2.053862, Domain acc: 0.489691, Label acc: 0.778007, Label loss: 0.616241 \n",
      "[[503  20   0   0   1]\n",
      " [222 842  77 103  12]\n",
      " [  1   3  61  11   8]\n",
      " [  0  13  32 502  27]\n",
      " [  7   3  23   2  47]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.69      0.96      0.80       524\n",
      "         N1       0.96      0.67      0.79      1256\n",
      "         N2       0.32      0.73      0.44        84\n",
      "         N3       0.81      0.87      0.84       574\n",
      "          R       0.49      0.57      0.53        82\n",
      "\n",
      "avg / total       0.83      0.78      0.78      2520\n",
      "\n",
      "0.775794\n",
      "Epoch number:  16\n",
      "TRAINING:   Total loss: 1.954154, Domain acc: 0.584833, Label acc: 0.782333, Label loss: 0.598590, Domain adapt para: 0.905148  lr: 0.000100\n",
      "VALIDATION: Total loss: 1.908808, Domain acc: 0.577148, Label acc: 0.801031, Label loss: 0.552199 \n",
      "[[503  18   2   0   1]\n",
      " [214 801 109  91  41]\n",
      " [  0   4  48  19  13]\n",
      " [  0  22  64 423  65]\n",
      " [  7   4  14   1  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.69      0.96      0.81       524\n",
      "         N1       0.94      0.64      0.76      1256\n",
      "         N2       0.20      0.57      0.30        84\n",
      "         N3       0.79      0.74      0.76       574\n",
      "          R       0.32      0.68      0.43        82\n",
      "\n",
      "avg / total       0.81      0.73      0.74      2520\n",
      "\n",
      "0.726587\n",
      "Epoch number:  17\n",
      "TRAINING:   Total loss: 1.907520, Domain acc: 0.576667, Label acc: 0.784667, Label loss: 0.579480, Domain adapt para: 0.921669  lr: 0.000100\n",
      "VALIDATION: Total loss: 1.942853, Domain acc: 0.531615, Label acc: 0.768729, Label loss: 0.607806 \n",
      "[[ 473   49    0    0    2]\n",
      " [ 124 1016   63   36   17]\n",
      " [   0    7   53    4   20]\n",
      " [   0   94   68  358   54]\n",
      " [   5    4   10    0   63]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.79      0.90      0.84       524\n",
      "         N1       0.87      0.81      0.84      1256\n",
      "         N2       0.27      0.63      0.38        84\n",
      "         N3       0.90      0.62      0.74       574\n",
      "          R       0.40      0.77      0.53        82\n",
      "\n",
      "avg / total       0.82      0.78      0.79      2520\n",
      "\n",
      "0.778968\n",
      "Epoch number:  18\n",
      "TRAINING:   Total loss: 1.824352, Domain acc: 0.594667, Label acc: 0.814000, Label loss: 0.529866, Domain adapt para: 0.935409  lr: 0.000100\n",
      "VALIDATION: Total loss: 1.837758, Domain acc: 0.590378, Label acc: 0.789347, Label loss: 0.562518 \n",
      "[[484  39   0   0   1]\n",
      " [134 952  90  60  20]\n",
      " [  0   4  53  10  17]\n",
      " [  0  27  63 428  56]\n",
      " [  6   3  12   1  60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.78      0.92      0.84       524\n",
      "         N1       0.93      0.76      0.83      1256\n",
      "         N2       0.24      0.63      0.35        84\n",
      "         N3       0.86      0.75      0.80       574\n",
      "          R       0.39      0.73      0.51        82\n",
      "\n",
      "avg / total       0.84      0.78      0.80      2520\n",
      "\n",
      "0.784524\n",
      "Epoch number:  19\n",
      "TRAINING:   Total loss: 1.825464, Domain acc: 0.582500, Label acc: 0.795667, Label loss: 0.556390, Domain adapt para: 0.946806  lr: 0.000100\n",
      "VALIDATION: Total loss: 1.816537, Domain acc: 0.551375, Label acc: 0.779038, Label loss: 0.562583 \n",
      "[[490  33   0   0   1]\n",
      " [162 900  88  83  23]\n",
      " [  1   3  53   8  19]\n",
      " [  0  11  60 441  62]\n",
      " [  8   3  14   1  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.74      0.94      0.83       524\n",
      "         N1       0.95      0.72      0.82      1256\n",
      "         N2       0.25      0.63      0.35        84\n",
      "         N3       0.83      0.77      0.80       574\n",
      "          R       0.35      0.68      0.46        82\n",
      "\n",
      "avg / total       0.83      0.77      0.79      2520\n",
      "\n",
      "0.769841\n",
      "Epoch number:  20\n",
      "TRAINING:   Total loss: 1.762753, Domain acc: 0.586000, Label acc: 0.806333, Label loss: 0.521308, Domain adapt para: 0.956237  lr: 0.000100\n",
      "VALIDATION: Total loss: 1.913334, Domain acc: 0.484021, Label acc: 0.775601, Label loss: 0.591272 \n",
      "[[489  31   1   2   1]\n",
      " [154 897 101  97   7]\n",
      " [  1   4  49   1  29]\n",
      " [  0   5  80 443  46]\n",
      " [  6   5  17   1  53]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.75      0.93      0.83       524\n",
      "         N1       0.95      0.71      0.82      1256\n",
      "         N2       0.20      0.58      0.30        84\n",
      "         N3       0.81      0.77      0.79       574\n",
      "          R       0.39      0.65      0.49        82\n",
      "\n",
      "avg / total       0.84      0.77      0.79      2520\n",
      "\n",
      "0.76627\n",
      "Epoch number:  21\n",
      "TRAINING:   Total loss: 1.724781, Domain acc: 0.579000, Label acc: 0.812333, Label loss: 0.495684, Domain adapt para: 0.964028  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.767214, Domain acc: 0.587457, Label acc: 0.801031, Label loss: 0.538137 \n",
      "[[ 471   51    1    0    1]\n",
      " [  97 1002   67   78   12]\n",
      " [   0    4   61    8   11]\n",
      " [   0   30   43  467   34]\n",
      " [   6    4   23    1   48]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.82      0.90      0.86       524\n",
      "         N1       0.92      0.80      0.85      1256\n",
      "         N2       0.31      0.73      0.44        84\n",
      "         N3       0.84      0.81      0.83       574\n",
      "          R       0.45      0.59      0.51        82\n",
      "\n",
      "avg / total       0.85      0.81      0.82      2520\n",
      "\n",
      "0.813095\n",
      "Epoch number:  22\n",
      "TRAINING:   Total loss: 1.704162, Domain acc: 0.596667, Label acc: 0.822667, Label loss: 0.493763, Domain adapt para: 0.970452  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.714177, Domain acc: 0.585567, Label acc: 0.811684, Label loss: 0.521317 \n",
      "[[495  28   0   0   1]\n",
      " [151 923  61  91  30]\n",
      " [  0   4  61   8  11]\n",
      " [  0  12  28 506  28]\n",
      " [  6   3  18   1  54]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.76      0.94      0.84       524\n",
      "         N1       0.95      0.73      0.83      1256\n",
      "         N2       0.36      0.73      0.48        84\n",
      "         N3       0.83      0.88      0.86       574\n",
      "          R       0.44      0.66      0.52        82\n",
      "\n",
      "avg / total       0.85      0.81      0.82      2520\n",
      "\n",
      "0.809127\n",
      "Epoch number:  23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING:   Total loss: 1.646616, Domain acc: 0.596167, Label acc: 0.840333, Label loss: 0.458627, Domain adapt para: 0.975743  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.716333, Domain acc: 0.528694, Label acc: 0.812715, Label loss: 0.500382 \n",
      "[[496  27   0   0   1]\n",
      " [126 944  97  68  21]\n",
      " [  0   4  66   4  10]\n",
      " [  0  22  56 449  47]\n",
      " [  6   4  26   1  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.79      0.95      0.86       524\n",
      "         N1       0.94      0.75      0.84      1256\n",
      "         N2       0.27      0.79      0.40        84\n",
      "         N3       0.86      0.78      0.82       574\n",
      "          R       0.36      0.55      0.44        82\n",
      "\n",
      "avg / total       0.85      0.79      0.81      2520\n",
      "\n",
      "0.793651\n",
      "Epoch number:  24\n",
      "TRAINING:   Total loss: 1.664517, Domain acc: 0.578667, Label acc: 0.831333, Label loss: 0.475621, Domain adapt para: 0.980096  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.736471, Domain acc: 0.489175, Label acc: 0.804124, Label loss: 0.517834 \n",
      "[[484  39   0   0   1]\n",
      " [123 955 104  59  15]\n",
      " [  0   4  68   3   9]\n",
      " [  0  29 129 374  42]\n",
      " [  6   4  25   0  47]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.79      0.92      0.85       524\n",
      "         N1       0.93      0.76      0.84      1256\n",
      "         N2       0.21      0.81      0.33        84\n",
      "         N3       0.86      0.65      0.74       574\n",
      "          R       0.41      0.57      0.48        82\n",
      "\n",
      "avg / total       0.84      0.77      0.79      2520\n",
      "\n",
      "0.765079\n",
      "Epoch number:  25\n",
      "TRAINING:   Total loss: 1.650755, Domain acc: 0.587000, Label acc: 0.830667, Label loss: 0.474898, Domain adapt para: 0.983675  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.742952, Domain acc: 0.536770, Label acc: 0.787629, Label loss: 0.554605 \n",
      "[[513   6   3   0   2]\n",
      " [296 686 171  73  30]\n",
      " [  1   2  56   5  20]\n",
      " [  0   3  51 424  96]\n",
      " [  6   3  15   1  57]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.63      0.98      0.77       524\n",
      "         N1       0.98      0.55      0.70      1256\n",
      "         N2       0.19      0.67      0.29        84\n",
      "         N3       0.84      0.74      0.79       574\n",
      "          R       0.28      0.70      0.40        82\n",
      "\n",
      "avg / total       0.83      0.69      0.71      2520\n",
      "\n",
      "0.688889\n",
      "Epoch number:  26\n",
      "TRAINING:   Total loss: 1.636766, Domain acc: 0.599167, Label acc: 0.829667, Label loss: 0.476764, Domain adapt para: 0.986614  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.762534, Domain acc: 0.514777, Label acc: 0.791753, Label loss: 0.568370 \n",
      "[[489  34   0   0   1]\n",
      " [113 956 118  52  17]\n",
      " [  0   4  68   2  10]\n",
      " [  0  38 129 364  43]\n",
      " [  6   3  28   0  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.80      0.93      0.86       524\n",
      "         N1       0.92      0.76      0.83      1256\n",
      "         N2       0.20      0.81      0.32        84\n",
      "         N3       0.87      0.63      0.73       574\n",
      "          R       0.39      0.55      0.45        82\n",
      "\n",
      "avg / total       0.85      0.76      0.79      2520\n",
      "\n",
      "0.762698\n",
      "Epoch number:  27\n",
      "TRAINING:   Total loss: 1.605879, Domain acc: 0.588167, Label acc: 0.838333, Label loss: 0.448120, Domain adapt para: 0.989027  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.753149, Domain acc: 0.517869, Label acc: 0.786598, Label loss: 0.589308 \n",
      "[[503  20   0   0   1]\n",
      " [184 805 143  97  27]\n",
      " [  1   3  70   1   9]\n",
      " [  0   0  86 447  41]\n",
      " [  6   3  22   0  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.72      0.96      0.83       524\n",
      "         N1       0.97      0.64      0.77      1256\n",
      "         N2       0.22      0.83      0.35        84\n",
      "         N3       0.82      0.78      0.80       574\n",
      "          R       0.40      0.62      0.48        82\n",
      "\n",
      "avg / total       0.84      0.74      0.77      2520\n",
      "\n",
      "0.744444\n",
      "Epoch number:  28\n",
      "TRAINING:   Total loss: 1.631208, Domain acc: 0.580000, Label acc: 0.824667, Label loss: 0.484356, Domain adapt para: 0.991007  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.725984, Domain acc: 0.508591, Label acc: 0.786942, Label loss: 0.551264 \n",
      "[[516   6   0   0   2]\n",
      " [321 693 106  99  37]\n",
      " [  1   3  64   0  16]\n",
      " [  0   2  83 404  85]\n",
      " [  8   3  15   0  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.61      0.98      0.75       524\n",
      "         N1       0.98      0.55      0.71      1256\n",
      "         N2       0.24      0.76      0.36        84\n",
      "         N3       0.80      0.70      0.75       574\n",
      "          R       0.29      0.68      0.40        82\n",
      "\n",
      "avg / total       0.82      0.69      0.70      2520\n",
      "\n",
      "0.687698\n",
      "Epoch number:  29\n",
      "TRAINING:   Total loss: 1.568457, Domain acc: 0.591833, Label acc: 0.848000, Label loss: 0.434032, Domain adapt para: 0.992632  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.659716, Domain acc: 0.581272, Label acc: 0.808248, Label loss: 0.526034 \n",
      "[[482  40   0   0   2]\n",
      " [ 97 992  89  67  11]\n",
      " [  0   4  69   1  10]\n",
      " [  0  39  95 387  53]\n",
      " [  6   5  20   0  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.82      0.92      0.87       524\n",
      "         N1       0.92      0.79      0.85      1256\n",
      "         N2       0.25      0.82      0.39        84\n",
      "         N3       0.85      0.67      0.75       574\n",
      "          R       0.40      0.62      0.49        82\n",
      "\n",
      "avg / total       0.84      0.79      0.80      2520\n",
      "\n",
      "0.786111\n",
      "Epoch number:  30\n",
      "TRAINING:   Total loss: 1.563220, Domain acc: 0.596500, Label acc: 0.849667, Label loss: 0.436559, Domain adapt para: 0.993963  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.707095, Domain acc: 0.528351, Label acc: 0.812028, Label loss: 0.511283 \n",
      "[[482  33   6   0   3]\n",
      " [ 89 879 143 118  27]\n",
      " [  0   4  75   3   2]\n",
      " [  0   5  66 482  21]\n",
      " [  2   3  45   2  30]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.84      0.92      0.88       524\n",
      "         N1       0.95      0.70      0.81      1256\n",
      "         N2       0.22      0.89      0.36        84\n",
      "         N3       0.80      0.84      0.82       574\n",
      "          R       0.36      0.37      0.36        82\n",
      "\n",
      "avg / total       0.85      0.77      0.79      2520\n",
      "\n",
      "0.773016\n",
      "Epoch number:  31\n",
      "TRAINING:   Total loss: 1.565720, Domain acc: 0.595167, Label acc: 0.838333, Label loss: 0.447931, Domain adapt para: 0.995055  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.662272, Domain acc: 0.509794, Label acc: 0.808248, Label loss: 0.507953 \n",
      "[[490  33   0   0   1]\n",
      " [108 902  95 131  20]\n",
      " [  0   4  63   7  10]\n",
      " [  0   3  46 497  28]\n",
      " [  6   4  22   2  48]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.81      0.94      0.87       524\n",
      "         N1       0.95      0.72      0.82      1256\n",
      "         N2       0.28      0.75      0.41        84\n",
      "         N3       0.78      0.87      0.82       574\n",
      "          R       0.45      0.59      0.51        82\n",
      "\n",
      "avg / total       0.85      0.79      0.81      2520\n",
      "\n",
      "0.793651\n",
      "Epoch number:  32\n",
      "TRAINING:   Total loss: 1.567342, Domain acc: 0.588500, Label acc: 0.837667, Label loss: 0.451852, Domain adapt para: 0.995949  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.756761, Domain acc: 0.509107, Label acc: 0.774571, Label loss: 0.600282 \n",
      "[[500  21   1   0   2]\n",
      " [136 871 154  53  42]\n",
      " [  0   4  64   0  16]\n",
      " [  0   8 152 326  88]\n",
      " [  6   4  13   0  59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.78      0.95      0.86       524\n",
      "         N1       0.96      0.69      0.80      1256\n",
      "         N2       0.17      0.76      0.27        84\n",
      "         N3       0.86      0.57      0.68       574\n",
      "          R       0.29      0.72      0.41        82\n",
      "\n",
      "avg / total       0.85      0.72      0.76      2520\n",
      "\n",
      "0.722222\n",
      "Epoch number:  33\n",
      "TRAINING:   Total loss: 1.561415, Domain acc: 0.579833, Label acc: 0.834333, Label loss: 0.449510, Domain adapt para: 0.996682  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.708259, Domain acc: 0.529210, Label acc: 0.812371, Label loss: 0.532307 \n",
      "[[482  38   0   0   4]\n",
      " [115 939  92  69  41]\n",
      " [  0   4  66   3  11]\n",
      " [  0   6  95 407  66]\n",
      " [  4   3  29   1  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.80      0.92      0.86       524\n",
      "         N1       0.95      0.75      0.84      1256\n",
      "         N2       0.23      0.79      0.36        84\n",
      "         N3       0.85      0.71      0.77       574\n",
      "          R       0.27      0.55      0.36        82\n",
      "\n",
      "avg / total       0.85      0.77      0.79      2520\n",
      "\n",
      "0.769445\n",
      "Epoch number:  34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING:   Total loss: 1.547587, Domain acc: 0.598833, Label acc: 0.842667, Label loss: 0.450726, Domain adapt para: 0.997283  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.676197, Domain acc: 0.540893, Label acc: 0.803780, Label loss: 0.546022 \n",
      "[[456  67   0   0   1]\n",
      " [ 93 967  79 115   2]\n",
      " [  0   5  66   7   6]\n",
      " [  0   2  54 491  27]\n",
      " [  6   4  26   1  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.82      0.87      0.85       524\n",
      "         N1       0.93      0.77      0.84      1256\n",
      "         N2       0.29      0.79      0.43        84\n",
      "         N3       0.80      0.86      0.83       574\n",
      "          R       0.56      0.55      0.55        82\n",
      "\n",
      "avg / total       0.84      0.80      0.82      2520\n",
      "\n",
      "0.803572\n",
      "Epoch number:  35\n",
      "TRAINING:   Total loss: 1.539884, Domain acc: 0.582667, Label acc: 0.841000, Label loss: 0.438102, Domain adapt para: 0.997775  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.694432, Domain acc: 0.520619, Label acc: 0.787973, Label loss: 0.575345 \n",
      "[[503  21   0   0   0]\n",
      " [176 885 123  64   8]\n",
      " [  1   3  72   0   8]\n",
      " [  0  22 116 390  46]\n",
      " [  7   4  20   0  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.73      0.96      0.83       524\n",
      "         N1       0.95      0.70      0.81      1256\n",
      "         N2       0.22      0.86      0.35        84\n",
      "         N3       0.86      0.68      0.76       574\n",
      "          R       0.45      0.62      0.52        82\n",
      "\n",
      "avg / total       0.84      0.75      0.78      2520\n",
      "\n",
      "0.754365\n",
      "Epoch number:  36\n",
      "TRAINING:   Total loss: 1.494529, Domain acc: 0.591167, Label acc: 0.857333, Label loss: 0.402487, Domain adapt para: 0.998178  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.689640, Domain acc: 0.446220, Label acc: 0.806529, Label loss: 0.532034 \n",
      "[[494  28   0   0   2]\n",
      " [132 914  66 122  22]\n",
      " [  0   5  63   3  13]\n",
      " [  0   2  66 455  51]\n",
      " [  6   4  22   1  49]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.78      0.94      0.85       524\n",
      "         N1       0.96      0.73      0.83      1256\n",
      "         N2       0.29      0.75      0.42        84\n",
      "         N3       0.78      0.79      0.79       574\n",
      "          R       0.36      0.60      0.45        82\n",
      "\n",
      "avg / total       0.84      0.78      0.80      2520\n",
      "\n",
      "0.78373\n",
      "Epoch number:  37\n",
      "TRAINING:   Total loss: 1.506556, Domain acc: 0.578833, Label acc: 0.847333, Label loss: 0.412780, Domain adapt para: 0.998508  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.630518, Domain acc: 0.520447, Label acc: 0.814089, Label loss: 0.516515 \n",
      "[[474  49   0   0   1]\n",
      " [ 91 988  85  83   9]\n",
      " [  0   4  65   3  12]\n",
      " [  1  12  61 462  38]\n",
      " [  7   4  20   0  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.83      0.90      0.86       524\n",
      "         N1       0.93      0.79      0.85      1256\n",
      "         N2       0.28      0.77      0.41        84\n",
      "         N3       0.84      0.80      0.82       574\n",
      "          R       0.46      0.62      0.53        82\n",
      "\n",
      "avg / total       0.85      0.81      0.82      2520\n",
      "\n",
      "0.809524\n",
      "Epoch number:  38\n",
      "TRAINING:   Total loss: 1.510499, Domain acc: 0.586000, Label acc: 0.845667, Label loss: 0.422600, Domain adapt para: 0.998778  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.609704, Domain acc: 0.524570, Label acc: 0.811340, Label loss: 0.514255 \n",
      "[[495  29   0   0   0]\n",
      " [114 991  71  52  28]\n",
      " [  0   7  68   0   9]\n",
      " [  0  40  74 380  80]\n",
      " [  6   7  19   0  50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.80      0.94      0.87       524\n",
      "         N1       0.92      0.79      0.85      1256\n",
      "         N2       0.29      0.81      0.43        84\n",
      "         N3       0.88      0.66      0.76       574\n",
      "          R       0.30      0.61      0.40        82\n",
      "\n",
      "avg / total       0.85      0.79      0.80      2520\n",
      "\n",
      "0.787302\n",
      "Epoch number:  39\n",
      "TRAINING:   Total loss: 1.505767, Domain acc: 0.593000, Label acc: 0.845333, Label loss: 0.425886, Domain adapt para: 0.999000  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.670951, Domain acc: 0.514777, Label acc: 0.790722, Label loss: 0.569595 \n",
      "[[ 426   92    5    0    1]\n",
      " [  47 1005  120   62   22]\n",
      " [   0    4   68    2   10]\n",
      " [   1   44  130  365   34]\n",
      " [   3    6   33    0   40]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.89      0.81      0.85       524\n",
      "         N1       0.87      0.80      0.84      1256\n",
      "         N2       0.19      0.81      0.31        84\n",
      "         N3       0.85      0.64      0.73       574\n",
      "          R       0.37      0.49      0.42        82\n",
      "\n",
      "avg / total       0.83      0.76      0.78      2520\n",
      "\n",
      "0.755556\n",
      "Epoch number:  40\n",
      "TRAINING:   Total loss: 1.484097, Domain acc: 0.588333, Label acc: 0.853000, Label loss: 0.410374, Domain adapt para: 0.999181  lr: 0.000050\n",
      "VALIDATION: Total loss: 1.749242, Domain acc: 0.534021, Label acc: 0.770447, Label loss: 0.631624 \n",
      "[[501  19   0   0   4]\n",
      " [189 806 158  56  47]\n",
      " [  0   4  56   1  23]\n",
      " [  0   5 170 281 118]\n",
      " [  5   2  15   0  60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.72      0.96      0.82       524\n",
      "         N1       0.96      0.64      0.77      1256\n",
      "         N2       0.14      0.67      0.23        84\n",
      "         N3       0.83      0.49      0.62       574\n",
      "          R       0.24      0.73      0.36        82\n",
      "\n",
      "avg / total       0.83      0.68      0.71      2520\n",
      "\n",
      "0.67619\n",
      "Epoch number:  41\n",
      "TRAINING:   Total loss: 1.423205, Domain acc: 0.613500, Label acc: 0.875000, Label loss: 0.362287, Domain adapt para: 0.999329  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.666458, Domain acc: 0.478866, Label acc: 0.805498, Label loss: 0.519801 \n",
      "[[496  26   0   0   2]\n",
      " [121 927  98  77  33]\n",
      " [  0   4  73   1   6]\n",
      " [  1   8  87 433  45]\n",
      " [  6   5  20   0  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.79      0.95      0.86       524\n",
      "         N1       0.96      0.74      0.83      1256\n",
      "         N2       0.26      0.87      0.40        84\n",
      "         N3       0.85      0.75      0.80       574\n",
      "          R       0.37      0.62      0.47        82\n",
      "\n",
      "avg / total       0.86      0.79      0.81      2520\n",
      "\n",
      "0.785714\n",
      "Epoch number:  42\n",
      "TRAINING:   Total loss: 1.432038, Domain acc: 0.590333, Label acc: 0.877667, Label loss: 0.358229, Domain adapt para: 0.999451  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.574363, Domain acc: 0.590034, Label acc: 0.813058, Label loss: 0.500741 \n",
      "[[490  32   0   0   2]\n",
      " [105 956  72  87  36]\n",
      " [  0   5  65   5   9]\n",
      " [  0   4  54 453  63]\n",
      " [  6   4  20   2  50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.82      0.94      0.87       524\n",
      "         N1       0.96      0.76      0.85      1256\n",
      "         N2       0.31      0.77      0.44        84\n",
      "         N3       0.83      0.79      0.81       574\n",
      "          R       0.31      0.61      0.41        82\n",
      "\n",
      "avg / total       0.85      0.80      0.82      2520\n",
      "\n",
      "0.799206\n",
      "Epoch number:  43\n",
      "TRAINING:   Total loss: 1.404307, Domain acc: 0.599167, Label acc: 0.885000, Label loss: 0.337323, Domain adapt para: 0.999550  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.592785, Domain acc: 0.588144, Label acc: 0.807904, Label loss: 0.516823 \n",
      "[[500  21   0   0   3]\n",
      " [145 852  88 116  55]\n",
      " [  0   4  71   2   7]\n",
      " [  0   1  50 448  75]\n",
      " [  6   4  19   2  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.77      0.95      0.85       524\n",
      "         N1       0.97      0.68      0.80      1256\n",
      "         N2       0.31      0.85      0.46        84\n",
      "         N3       0.79      0.78      0.78       574\n",
      "          R       0.27      0.62      0.37        82\n",
      "\n",
      "avg / total       0.84      0.76      0.78      2520\n",
      "\n",
      "0.762698\n",
      "Epoch number:  44\n",
      "TRAINING:   Total loss: 1.378520, Domain acc: 0.588833, Label acc: 0.891000, Label loss: 0.302455, Domain adapt para: 0.999632  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.611280, Domain acc: 0.506701, Label acc: 0.814433, Label loss: 0.514998 \n",
      "[[503  19   0   0   2]\n",
      " [162 880  72  92  50]\n",
      " [  0   4  68   5   7]\n",
      " [  0   2  36 466  70]\n",
      " [  6   5  19   2  50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.75      0.96      0.84       524\n",
      "         N1       0.97      0.70      0.81      1256\n",
      "         N2       0.35      0.81      0.49        84\n",
      "         N3       0.82      0.81      0.82       574\n",
      "          R       0.28      0.61      0.38        82\n",
      "\n",
      "avg / total       0.85      0.78      0.80      2520\n",
      "\n",
      "0.780556\n",
      "Epoch number:  45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING:   Total loss: 1.399251, Domain acc: 0.594667, Label acc: 0.887667, Label loss: 0.328436, Domain adapt para: 0.999699  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.637979, Domain acc: 0.541581, Label acc: 0.802406, Label loss: 0.538841 \n",
      "[[498  24   0   0   2]\n",
      " [126 892 117  68  53]\n",
      " [  0   4  71   0   9]\n",
      " [  1  13  97 373  90]\n",
      " [  6   5  22   1  48]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.79      0.95      0.86       524\n",
      "         N1       0.95      0.71      0.81      1256\n",
      "         N2       0.23      0.85      0.36        84\n",
      "         N3       0.84      0.65      0.73       574\n",
      "          R       0.24      0.59      0.34        82\n",
      "\n",
      "avg / total       0.85      0.75      0.77      2520\n",
      "\n",
      "0.746825\n",
      "Epoch number:  46\n",
      "TRAINING:   Total loss: 1.379380, Domain acc: 0.593000, Label acc: 0.891667, Label loss: 0.309067, Domain adapt para: 0.999753  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.642606, Domain acc: 0.485395, Label acc: 0.814433, Label loss: 0.519997 \n",
      "[[501  21   0   0   2]\n",
      " [145 894  85  82  50]\n",
      " [  0   5  71   2   6]\n",
      " [  1  11  68 434  60]\n",
      " [  6   5  24   3  44]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.77      0.96      0.85       524\n",
      "         N1       0.96      0.71      0.82      1256\n",
      "         N2       0.29      0.85      0.43        84\n",
      "         N3       0.83      0.76      0.79       574\n",
      "          R       0.27      0.54      0.36        82\n",
      "\n",
      "avg / total       0.84      0.77      0.79      2520\n",
      "\n",
      "0.771429\n",
      "Epoch number:  47\n",
      "TRAINING:   Total loss: 1.370764, Domain acc: 0.603000, Label acc: 0.894333, Label loss: 0.301858, Domain adapt para: 0.999798  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.600154, Domain acc: 0.607560, Label acc: 0.810997, Label loss: 0.521189 \n",
      "[[500  18   0   0   6]\n",
      " [154 868 100  82  52]\n",
      " [  0   4  71   1   8]\n",
      " [  0   4  85 387  98]\n",
      " [  6   4  21   1  50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.76      0.95      0.84       524\n",
      "         N1       0.97      0.69      0.81      1256\n",
      "         N2       0.26      0.85      0.39        84\n",
      "         N3       0.82      0.67      0.74       574\n",
      "          R       0.23      0.61      0.34        82\n",
      "\n",
      "avg / total       0.84      0.74      0.77      2520\n",
      "\n",
      "0.744445\n",
      "Epoch number:  48\n",
      "TRAINING:   Total loss: 1.375908, Domain acc: 0.589500, Label acc: 0.888667, Label loss: 0.305313, Domain adapt para: 0.999835  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.636507, Domain acc: 0.536942, Label acc: 0.804124, Label loss: 0.537764 \n",
      "[[498  25   0   0   1]\n",
      " [135 923  74  71  53]\n",
      " [  0   6  72   1   5]\n",
      " [  0  12  56 407  99]\n",
      " [  6   4  28   3  41]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.78      0.95      0.86       524\n",
      "         N1       0.95      0.73      0.83      1256\n",
      "         N2       0.31      0.86      0.46        84\n",
      "         N3       0.84      0.71      0.77       574\n",
      "          R       0.21      0.50      0.29        82\n",
      "\n",
      "avg / total       0.85      0.77      0.79      2520\n",
      "\n",
      "0.770238\n",
      "Epoch number:  49\n",
      "TRAINING:   Total loss: 1.366146, Domain acc: 0.598167, Label acc: 0.887000, Label loss: 0.300533, Domain adapt para: 0.999865  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.621749, Domain acc: 0.571821, Label acc: 0.804467, Label loss: 0.540657 \n",
      "[[503  18   0   0   3]\n",
      " [154 840 112  87  63]\n",
      " [  0   4  63   1  16]\n",
      " [  0   1  78 362 133]\n",
      " [  6   5  18   0  53]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.76      0.96      0.85       524\n",
      "         N1       0.97      0.67      0.79      1256\n",
      "         N2       0.23      0.75      0.35        84\n",
      "         N3       0.80      0.63      0.71       574\n",
      "          R       0.20      0.65      0.30        82\n",
      "\n",
      "avg / total       0.84      0.72      0.75      2520\n",
      "\n",
      "0.722619\n",
      "Epoch number:  50\n",
      "TRAINING:   Total loss: 1.374517, Domain acc: 0.588000, Label acc: 0.895000, Label loss: 0.305528, Domain adapt para: 0.999889  lr: 0.000010\n",
      "VALIDATION: Total loss: 1.592563, Domain acc: 0.581615, Label acc: 0.820275, Label loss: 0.520647 \n",
      "[[494  28   0   0   2]\n",
      " [111 958  76  77  34]\n",
      " [  0   6  70   1   7]\n",
      " [  0  12  91 380  91]\n",
      " [  6   5  25   0  46]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          W       0.81      0.94      0.87       524\n",
      "         N1       0.95      0.76      0.85      1256\n",
      "         N2       0.27      0.83      0.40        84\n",
      "         N3       0.83      0.66      0.74       574\n",
      "          R       0.26      0.56      0.35        82\n",
      "\n",
      "avg / total       0.85      0.77      0.80      2520\n",
      "\n",
      "0.773016\n"
     ]
    }
   ],
   "source": [
    "noEpochs = 50\n",
    "\n",
    "def train_and_evaluate(training_mode, graph, model, verbose=1):\n",
    "    \"\"\"Helper to run the model with different training modes.\"\"\"\n",
    "    _domain_loss_train, _domain_acc_train, _label_loss_train, _label_acc_train, _total_loss_train = [], [], [], [], []\n",
    "    \n",
    "    domain_loss_train, domain_loss_test, domain_loss_valid = [], [], []\n",
    "    domain_acc_train, domain_acc_test, domain_acc_valid = [], [], []\n",
    "    label_loss_train, label_loss_test, label_loss_valid = [], [], []\n",
    "    label_acc_train, label_acc_test, label_acc_test_phys, label_acc_test_hosp, label_acc_valid = [], [], [], [], []\n",
    "    total_loss_train, total_loss_valid = [], []\n",
    "    \n",
    "    domain_labels = np.vstack([np.tile([1, 0], [batch_size // 2, 1]),\n",
    "                                np.tile([0, 1], [batch_size // 2, 1])]) \n",
    "    \n",
    "    with tf.Session(graph=graph, config=config) as sess:\n",
    "        initialize_uninitialized(sess)\n",
    "        \n",
    "        if training_mode == 'dann':\n",
    "            saverDanet = tf.train.Saver()\n",
    "            trainFile = open('trainPerformanceDann.txt', 'w')\n",
    "            validFile = open('validPerformanceDann.txt', 'w')\n",
    "            testFile = open('testPerformanceDann.txt', 'w')\n",
    "        elif training_mode == 'source':\n",
    "            saverSource = tf.train.Saver()\n",
    "            trainFile = open('trainPerformanceSource.txt', 'w')\n",
    "            validFile = open('validPerformanceSource.txt', 'w')\n",
    "            testFile = open('testPerformanceSource.txt', 'w')\n",
    "        else:\n",
    "            saverTarget = tf.train.Saver()\n",
    "            trainFile = open('trainPerformanceTarget.txt', 'w')\n",
    "            validFile = open('validPerformanceTarget.txt', 'w')\n",
    "            testFile = open('testPerformanceTarget.txt', 'w')\n",
    "        \n",
    "        # Training loop\n",
    "        for ii in range(noEpochs):\n",
    "            print(\"Epoch number: \", ii+1)\n",
    "            inputs_tr_phys,targets_tr_phys = get_data_training_epoch(inputs_train_phys,targets_train_phys)\n",
    "            inputs_tr_hosp, targets_tr_hosp = get_data_training_epoch(inputs_train_hosp, targets_train_hosp)\n",
    "  \n",
    "            # Adaptation param and learning rate schedule as described in the paper\n",
    "            p = float(ii) / noEpochs\n",
    "            l = 2. / (1. + np.exp(-10. * p)) - 1\n",
    "#             lr = 0.01 / ((1. + 10 * p)**0.75)\n",
    "#             lr = 0.0001\n",
    "            if ii < 20:\n",
    "                lr = 0.0001\n",
    "            elif ii >= 20 and ii < 40:\n",
    "                lr = 0.00005\n",
    "            else:\n",
    "                lr = 0.00001\n",
    "                    \n",
    "            # Training step\n",
    "            thisNo = 0\n",
    "            if training_mode == 'dann':\n",
    "                ################## TRAINING ########################\n",
    "                for batch in iterate_minibatches_dann(inputs_tr_phys, targets_tr_phys, inputs_tr_hosp, \n",
    "                                                      targets_tr_hosp, batch_size, shuffle=True):               \n",
    "                    X, y = batch #Get the values \n",
    "                    if len(y) == batch_size:\n",
    "\n",
    "                        thisNo += 1\n",
    "                        _, batch_loss, dloss, ploss, d_acc, p_acc = \\\n",
    "                            sess.run([dann_train_op, total_loss, domain_loss, pred_loss, domain_acc, label_acc],\n",
    "                                     feed_dict={model.X: X, model.y: y, model.domain: domain_labels,\n",
    "                                                model.train: True, model.l: l, learning_rate: lr})\n",
    "                        _domain_loss_train.append(dloss)\n",
    "                        _domain_acc_train.append(d_acc)\n",
    "                        _label_loss_train.append(ploss)\n",
    "                        _label_acc_train.append(p_acc)\n",
    "                        _total_loss_train.append(batch_loss)\n",
    "                        \n",
    "                # Append for each epoch\n",
    "                domain_loss_train.append(np.mean(_domain_loss_train))\n",
    "                domain_acc_train.append(np.mean(_domain_acc_train))\n",
    "                label_loss_train.append(np.mean(_label_loss_train))\n",
    "                label_acc_train.append(np.mean(_label_acc_train))\n",
    "                total_loss_train.append(np.mean(_total_loss_train))\n",
    "                print('TRAINING:   Total loss: %f, Domain acc: %f, Label acc: %f, Label loss: %f, Domain adapt para: %f  lr: %f' % \\\n",
    "                      (np.mean(_total_loss_train), np.mean(_domain_acc_train), np.mean(_label_acc_train), np.mean(_label_loss_train), l, lr))\n",
    "\n",
    "                trainFile.write(\"%s\\n\" % [np.mean(_total_loss_train), np.mean(_domain_acc_train), np.mean(_label_acc_train), np.mean(_label_loss_train), l, lr])\n",
    "                _domain_loss_train, _domain_acc_train, _label_loss_train, _label_acc_train, _total_loss_train = [], [], [], [], []\n",
    "                ################## TRAINING ENDED ##########################\n",
    "                \n",
    "                ################## VALIDATION ########################\n",
    "                _domain_loss_valid, _domain_acc_valid, _label_loss_valid, _label_acc_valid, _total_loss_valid = [], [], [], [], []\n",
    "                batchNo = 0\n",
    "                for batch in iterate_minibatches_dann(inputs_val_phys, targets_val_phys, inputs_val_hosp,\n",
    "                                          targets_val_hosp, batch_size, shuffle=False):                \n",
    "                    X, y = batch  # Get the values\n",
    "                    if len(y) == batch_size:\n",
    "                        batchNo += 1\n",
    "                        # Feed_dict and fetches\n",
    "                        feed_dict = {model.X: X, model.y: y, model.domain: domain_labels, model.train: True}\n",
    "                        fetches = [total_loss, domain_loss, pred_loss, domain_acc, label_acc]\n",
    "                        batch_loss, dloss, ploss, d_acc, p_acc = sess.run(fetches, feed_dict=feed_dict)\n",
    "                        # Get the loss for a single batch averaged\n",
    "\n",
    "                        _domain_loss_valid.append(dloss)\n",
    "                        _domain_acc_valid.append(d_acc)\n",
    "                        _label_loss_valid.append(ploss)\n",
    "                        _label_acc_valid.append(p_acc)\n",
    "                        _total_loss_valid.append(batch_loss)\n",
    "                # Append loss and accuracy\n",
    "                label_loss_valid.append(np.mean(_label_loss_valid))\n",
    "                label_acc_valid.append(np.mean(_label_acc_valid))\n",
    "                domain_loss_valid.append(np.mean(_domain_loss_valid))\n",
    "                domain_acc_valid.append(np.mean(_domain_acc_valid))\n",
    "                total_loss_valid.append(np.mean(_total_loss_valid))\n",
    "                ################## VALIDATION ENDED ########################\n",
    "                print('VALIDATION: Total loss: %f, Domain acc: %f, Label acc: %f, Label loss: %f ' % \\\n",
    "                  (np.mean(_total_loss_valid), np.mean(_domain_acc_valid), np.mean(_label_acc_valid), np.mean(_label_loss_valid)))\n",
    "            \n",
    "                validFile.write(\"%s\\n\" % [np.mean(_total_loss_valid), np.mean(_domain_acc_valid), np.mean(_label_acc_valid), np.mean(_label_loss_valid)])        \n",
    "                ################## TESTING ########################\n",
    "                # IS FURHTER BELOW                  \n",
    "            elif training_mode == 'source':\n",
    "                batchNo = 0 \n",
    "                ################## TRAINING ########################\n",
    "                for batch in iterate_minibatches(inputs_tr_phys, targets_tr_phys, batch_size, shuffle=True):\n",
    "                    X, y = batch\n",
    "                    if len(y) == batch_size:\n",
    "                        batchNo += 1\n",
    "                        feed_dict = {model.X: X, model.y: y, model.train: False, model.l: l, learning_rate: lr}\n",
    "                        fetches = [regular_train_op, pred_loss, label_acc]\n",
    "                        _, ploss, p_acc = sess.run(fetches, feed_dict=feed_dict)\n",
    "                        _label_loss_train.append(ploss)\n",
    "                        _label_acc_train.append(p_acc)\n",
    "                        \n",
    "                # Append for each epoch\n",
    "                label_loss_train.append(np.mean(_label_loss_train))\n",
    "                label_acc_train.append(np.mean(_label_acc_train))\n",
    "                print('TRAINING:   Label acc: %f, Label loss: %f, lr: %f' % \\\n",
    "                      (np.mean(_label_acc_train), np.mean(_label_loss_train), lr))\n",
    "\n",
    "                trainFile.write(\"%s\\n\" % [np.mean(_label_acc_train), np.mean(_label_loss_train), lr])\n",
    "                _label_loss_train, _label_acc_train = [], []\n",
    "                ################## TRAINING ENDED ########################\n",
    "                ################## VALIDATION ########################\n",
    "                _label_loss_valid, _label_acc_valid = [], []\n",
    "                batchNo = 0\n",
    "                for batch in iterate_minibatches(inputs_val_phys, targets_val_phys, batch_size, shuffle=False):                \n",
    "                    X, y = batch  # Get the values\n",
    "                    if len(y) == batch_size:\n",
    "                        batchNo += 1\n",
    "                        # Feed_dict and fetches\n",
    "                        feed_dict = {model.X: X, model.y: y, model.train: False}\n",
    "                        fetches = [pred_loss, label_acc]\n",
    "                        ploss, p_acc = sess.run(fetches, feed_dict=feed_dict)\n",
    "                        # Get the loss for a single batch averaged\n",
    "                        _label_loss_valid.append(ploss)\n",
    "                        _label_acc_valid.append(p_acc)\n",
    "                        \n",
    "                # Append loss and accuracy\n",
    "                label_loss_valid.append(np.mean(_label_loss_valid))\n",
    "                label_acc_valid.append(np.mean(_label_acc_valid))\n",
    "                print('VALIDATION: Label acc: %f, Label loss: %f ' % (np.mean(_label_acc_valid), np.mean(_label_loss_valid)))\n",
    "            \n",
    "                validFile.write(\"%s\\n\" % [np.mean(_label_acc_valid), np.mean(_label_loss_valid)])    \n",
    "                ################## VALIDATION ENDED ########################\n",
    "    \n",
    "                ################## TESTING ########################\n",
    "                # IS FURHTER BELOW      \n",
    "            elif training_mode == 'target':\n",
    "                ################## TRAINING ########################\n",
    "                batchNo = 0\n",
    "                for batch in iterate_minibatches(inputs_tr_hosp, targets_tr_hosp, batch_size, shuffle=True):\n",
    "                    X, y = batch\n",
    "                    if len(y) == batch_size:\n",
    "                        feed_dict = {model.X: X, model.y: y, model.train: False, model.l: l, learning_rate: lr}\n",
    "                        fetches = [regular_train_op, pred_loss, label_acc]\n",
    "                        _, ploss, p_acc = sess.run(fetches, feed_dict=feed_dict)\n",
    "                        _label_loss_train.append(ploss)\n",
    "                        _label_acc_train.append(p_acc)\n",
    "                        \n",
    "                # Append for each epoch\n",
    "                label_loss_train.append(np.mean(_label_loss_train))\n",
    "                label_acc_train.append(np.mean(_label_acc_train))\n",
    "                print('TRAINING:   Label acc: %f, Label loss: %f, lr: %f' % \\\n",
    "                      (np.mean(_label_acc_train), np.mean(_label_loss_train), lr))\n",
    "\n",
    "                trainFile.write(\"%s\\n\" % [np.mean(_label_acc_train), np.mean(_label_loss_train), lr])\n",
    "                _label_loss_train, _label_acc_train = [], []\n",
    "                ################## TRAINING ENDED ########################\n",
    "                \n",
    "                ################## VALIDATION ########################\n",
    "                _label_loss_valid, _label_acc_valid = [], []\n",
    "                batchNo = 0\n",
    "                for batch in iterate_minibatches(inputs_val_hosp, targets_val_hosp, batch_size, shuffle=False):                \n",
    "                    X, y = batch  # Get the values\n",
    "                    if len(y) == batch_size:\n",
    "                        batchNo += 1\n",
    "                        # Feed_dict and fetches\n",
    "                        feed_dict = {model.X: X, model.y: y, model.train: False}\n",
    "                        fetches = [pred_loss, label_acc]\n",
    "                        ploss, p_acc = sess.run(fetches, feed_dict=feed_dict)\n",
    "                        # Get the loss for a single batch averaged\n",
    "                        _label_loss_valid.append(ploss)\n",
    "                        _label_acc_valid.append(p_acc)\n",
    "                        \n",
    "                # Append loss and accuracy\n",
    "                label_loss_valid.append(np.mean(_label_loss_valid))\n",
    "                label_acc_valid.append(np.mean(_label_acc_valid))\n",
    "                print('VALIDATION: Label acc: %f, Label loss: %f ' % (np.mean(_label_acc_valid), np.mean(_label_loss_valid)))\n",
    "            \n",
    "                validFile.write(\"%s\\n\" % [np.mean(_label_acc_valid), np.mean(_label_loss_valid)])    \n",
    "                ################## VALIDATION ENDED ########################\n",
    "               \n",
    "                ################## TESTING ON SOURCE DATA ########################\n",
    "                _temp_pred = []\n",
    "                _temp_target = []\n",
    "                _label_acc_test = []\n",
    "                batchNo = 0\n",
    "                for batch in iterate_minibatches(inputs_test_hosp, targets_test_hosp, batch_size, shuffle=False):\n",
    "                    X, y = batch  # Get the values\n",
    "                    if len(y) == batch_size:\n",
    "                        batchNo += 1\n",
    "\n",
    "                        # Feed_dict and fetches\n",
    "                        feed_dict = {model.X: X, model.y: y, model.train: False}\n",
    "                        fetches = [label_acc, model.pred]\n",
    "                        p_acc, prediction = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "                        _temp_pred = np.append(_temp_pred, np.argmax(prediction, axis=1))\n",
    "\n",
    "                        # Convert targets from onehot\n",
    "                        _temp_target = np.append(_temp_target, [np.where(r == 1)[0][0] for r in y])\n",
    "                        _label_acc_test.append(p_acc)\n",
    "\n",
    "                # Confusion matrix\n",
    "                conf_mat = confusion_matrix(_temp_target, _temp_pred)\n",
    "                print(conf_mat)\n",
    "\n",
    "                # Per class metrics\n",
    "                class_report = classification_report(_temp_target, _temp_pred, target_names=target_names)\n",
    "                print(class_report)\n",
    "\n",
    "                print(np.mean(_label_acc_test))\n",
    "\n",
    "                testFile.write(\"%s\\n\" % [conf_mat])\n",
    "                testFile.write(\"%s\\n\" % [class_report])\n",
    "                testFile.write(\"%s\\n\" % [np.mean(_label_acc_test)])\n",
    "            \n",
    "            if training_mode == 'dann' or training_mode == 'source':\n",
    "                _temp_pred = []\n",
    "                _temp_target = []\n",
    "                _label_acc_test = []\n",
    "                batchNo = 0\n",
    "                for batch in iterate_minibatches(inputs_test_hosp, targets_test_hosp, batch_size, shuffle=False):\n",
    "                    X, y = batch  # Get the values\n",
    "                    if len(y) == batch_size:\n",
    "                        batchNo += 1\n",
    "\n",
    "                        # Feed_dict and fetches\n",
    "                        feed_dict = {model.X: X, model.y: y, model.train: False}\n",
    "                        fetches = [label_acc, model.pred]\n",
    "                        p_acc, prediction = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "                        _temp_pred = np.append(_temp_pred, np.argmax(prediction, axis=1))\n",
    "\n",
    "                        # Convert targets from onehot\n",
    "                        _temp_target = np.append(_temp_target, [np.where(r == 1)[0][0] for r in y])\n",
    "                        _label_acc_test.append(p_acc)\n",
    "\n",
    "                # Confusion matrix\n",
    "                conf_mat = confusion_matrix(_temp_target, _temp_pred)\n",
    "                print(conf_mat)\n",
    "\n",
    "                # Per class metrics\n",
    "                class_report = classification_report(_temp_target, _temp_pred, target_names=target_names)\n",
    "                print(class_report)\n",
    "\n",
    "                print(np.mean(_label_acc_test))\n",
    "\n",
    "                testFile.write(\"%s\\n\" % [conf_mat])\n",
    "                testFile.write(\"%s\\n\" % [class_report])\n",
    "                testFile.write(\"%s\\n\" % [np.mean(_label_acc_test)])\n",
    "        \n",
    "        # Get the outputs\n",
    "        test_emb = sess.run(model.feature, feed_dict={model.X: combined_test_imgs})\n",
    "        testFile.close()\n",
    "        validFile.close()\n",
    "        testFile.close()\n",
    "        \n",
    "        \n",
    "        if training_mode == 'target':\n",
    "            saverTarget.save(sess, \"/home/jaskmo/Documents/programering/02456-deep-learning/Project/RES/model_target.ckpt\")\n",
    "        elif training_mode == 'source':\n",
    "            saverSource.save(sess, \"/home/jaskmo/Documents/programering/02456-deep-learning/Project/RES/model_source.ckpt\")\n",
    "        else:\n",
    "            saverDanet.save(sess, \"/home/jaskmo/Documents/programering/02456-deep-learning/Project/RES/model_danet.ckpt\")\n",
    "                \n",
    "        return test_emb   \n",
    "\n",
    "\n",
    "#print('\\nSource only training')\n",
    "#source_emb = train_and_evaluate('source', graph, model)\n",
    "\n",
    "#print('\\nTarget only training')\n",
    "#target_emb = train_and_evaluate('target', graph, model)\n",
    "\n",
    "print('\\nDomain adaptation training')\n",
    "dann_emb = train_and_evaluate('dann', graph, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}