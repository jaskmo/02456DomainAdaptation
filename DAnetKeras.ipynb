{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import CSVLogger, Callback\n",
    "from datetime import datetime\n",
    "import keras.backend as K\n",
    "import extras.ourUtils\n",
    "import numpy as np\n",
    "import Models\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "nrEpochs = 10\n",
    "full_train = True\n",
    "path = '/home/jaskmo/Documents/programering/02456DomainAdaptation/'\n",
    "source_data = path + 'taperImages/pysNetData'\n",
    "target_data = path + 'taperImages/hData'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data as generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# make a data generator for dplInput\n",
    "def train_gen_DAnet(source, target, batch_size):\n",
    "    half = batch_size//2\n",
    "    while True:\n",
    "        source_data, source_lable = source.next()\n",
    "        target_data, target_lable = target.next()\n",
    "        if len(source_lable) != batch_size or len(target_lable) != batch_size:\n",
    "            continue\n",
    "        dpl_data = np.concatenate((source_data[:half,...],target_data[:half,...]),axis=0)\n",
    "               \n",
    "        domain_tmp = np.ones(batch_size, dtype='int8')\n",
    "        domain_tmp[half:] = domain_tmp[half:] * 0\n",
    "        dpl_lable = np.concatenate((domain_tmp.reshape(batch_size,1),\n",
    "                                       np.flip(domain_tmp,0).reshape(batch_size,1)),1)\n",
    "\n",
    "        yield({'lplInput':source_data,'dplInput':dpl_data}, {'lplOut':source_lable,'dplOut':dpl_lable})\n",
    "        \n",
    "def test_gen_DAnet(source, target, batch_size):\n",
    "    half = batch_size//2\n",
    "    while True:\n",
    "        source_data, source_lable = source.next()\n",
    "        target_data, target_lable = target.next()\n",
    "        if len(source_lable) != batch_size or len(target_lable) != batch_size:\n",
    "            continue\n",
    "        dpl_data = np.concatenate((source_data[:half,...],target_data[:half,...]),axis=0)\n",
    "               \n",
    "        domain_tmp = np.ones(batch_size, dtype='int8')\n",
    "        domain_tmp[half:] = domain_tmp[half:] * 0\n",
    "        dpl_lable = np.concatenate((domain_tmp.reshape(batch_size,1),\n",
    "                                       np.flip(domain_tmp,0).reshape(batch_size,1)),1)\n",
    "\n",
    "        yield({'lplInput':target_data,'dplInput':dpl_data}, {'lplOut':target_lable,'dplOut':dpl_lable})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29772 images belonging to 5 classes.\n",
      "Found 10629 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_gen_source = datagen.flow_from_directory(source_data + '/train', target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "train_gen_target = datagen.flow_from_directory(target_data + '/train', target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "train_gen_DA = train_gen_DAnet(train_gen_source, train_gen_target, batch_size)\n",
    "\n",
    "train_stepE = np.floor_divide(train_gen_source.n, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4807 images belonging to 5 classes.\n",
      "Found 2838 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_gen_source = datagen.flow_from_directory(source_data + '/validation', target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "valid_gen_target = datagen.flow_from_directory(target_data + '/validation', target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "valid_gen_DA = test_gen_DAnet(valid_gen_source, valid_gen_target, batch_size)\n",
    "\n",
    "val_stepE = np.floor_divide(valid_gen_source.n, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3862 images belonging to 5 classes.\n",
      "Found 2722 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen_source = datagen.flow_from_directory(source_data + '/test', target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "test_gen_target = datagen.flow_from_directory(target_data + '/test', target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "test_gen_DA = test_gen_DAnet(test_gen_source, test_gen_target, batch_size)\n",
    "\n",
    "test_stepE = np.floor_divide(test_gen_source.n, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lplInput (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dplInput (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 25088)         14714688    lplInput[0][0]                   \n",
      "                                                                   dplInput[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flipGrad (Lambda)                (None, 25088)         0           model_1[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lpl1 (Dense)                     (None, 2048)          51382272    model_1[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dpl1 (Dense)                     (None, 2048)          51382272    flipGrad[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                 (None, 2048)          0           lpl1[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                 (None, 2048)          0           dpl1[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "lpl2 (Dense)                     (None, 1024)          2098176     lpl1Do[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dpl2 (Dense)                     (None, 1024)          2098176     dpl1Do[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lplOut (Dense)                   (None, 5)             5125        lpl2[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "dplOut (Dense)                   (None, 2)             2050        dpl2[0][0]                       \n",
      "====================================================================================================\n",
      "Total params: 121,682,759\n",
      "Trainable params: 121,682,759\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# init. the variable to controle the flipgradient layer\n",
    "lamFunk = K.variable(0.0)\n",
    "current_model = Models.DA_model(lamFunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "csv_logger = CSVLogger('/media/jaskmo/ELEK/bme/Project02456/trainingLog/DA_Model' + \n",
    "                           str(now.day) + '-' + str(now.month) + '-' + str(now.year) + '_' + \n",
    "                           str(now.hour) + str(now.minute) + '.log')\n",
    "\n",
    "class FlipControle(Callback):\n",
    "    def __init__(self, alphaIn):\n",
    "        self.alpha = alphaIn\n",
    "        print(K.get_value(lamFunk))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        p = (epoch+1)/nrEpochs\n",
    "        K.set_value(self.alpha, (2/(1+np.exp(-10*p)))-1)\n",
    "        print(K.get_value(lamFunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the S!@Â¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Epoch 1/10\n",
      "1487/1488 [============================>.] - ETA: 0s - loss: 40.3956 - lplOut_loss: 0.4717 - dplOut_loss: 0.1508 - lplOut_categorical_accuracy: 0.8246 - dplOut_categorical_accuracy: 0.94100.462117\n",
      "1488/1488 [==============================] - 1213s - loss: 40.3824 - lplOut_loss: 0.4716 - dplOut_loss: 0.1508 - lplOut_categorical_accuracy: 0.8247 - dplOut_categorical_accuracy: 0.9410 - val_loss: 21.1235 - val_lplOut_loss: 0.4061 - val_dplOut_loss: 0.4954 - val_lplOut_categorical_accuracy: 0.8642 - val_dplOut_categorical_accuracy: 0.8304\n",
      "Epoch 2/10\n",
      "  98/1488 [>.............................] - ETA: 1084s - loss: 32.5881 - lplOut_loss: 6.9680 - dplOut_loss: 5.9115 - lplOut_categorical_accuracy: 0.4444 - dplOut_categorical_accuracy: 0.5250"
     ]
    }
   ],
   "source": [
    "current_model.fit_generator(train_gen_DA, train_stepE, epochs=nrEpochs, verbose=1, validation_data=test_gen_DA, \n",
    "                            validation_steps=val_stepE, callbacks=[csv_logger,FlipControle(lamFunk)], initial_epoch=0,\n",
    "                            max_queue_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "current_model.save(filepath='/home/jaskmo/Documents/programering/02456DomainAdaptation/models/kerasDA' + \n",
    "                  str(now.day) + '-' + str(now.month) + '-' + str(now.year) + '_' + \n",
    "                  str(now.hour) + str(now.minute) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_img, test_lable = test_gen_target.next()\n",
    "for count in range(test_gen_target.n//batch_size):\n",
    "    tmp_img, tmp_lable = test_gen_target.next()\n",
    "    test_img = np.concatenate((test_img, tmp_img), axis=0)\n",
    "    test_lable = np.concatenate((test_lable, tmp_lable),axis=0)\n",
    "    \n",
    "fluke = np.zeros(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, metric = current_model.evaluate(x=inputs_test_phys, y=targets_test_phys, batch_size=50)\n",
    "inv_map = {v: k for k, v in test_gen_target.class_indices.items()}\n",
    "target_names = list(inv_map.values())\n",
    "\n",
    "targets_test_int = [np.where(r == 1)[0][0] for r in test_lable]\n",
    "y_pred = current_model.predict([test_img,fluke])\n",
    "y_pred2 = np.argmax(y_pred[0], axis = 1)\n",
    "# Test accuracy:\n",
    "acc = accuracy_score(targets_test_int, y_pred2)\n",
    "print('Accuracy on target domain = ', acc)\n",
    "\n",
    "conf_mat = confusion_matrix(targets_test_int, y_pred2)\n",
    "print(conf_mat)\n",
    "# Per class metrics\n",
    "class_report = classification_report(targets_test_int, y_pred2, target_names=target_names)\n",
    "print(class_report)\n",
    "\n",
    "# Evaluate error on source data\n",
    "# _, metric = current_model.evaluate_generator(generator=test_gen_DA, steps=test_stepE)\n",
    "# print('Accuracy on source domain = ', metric)\n",
    "\n",
    "    \n",
    "# elif training_mode == 'target': # Training on target data from hospital\n",
    "#     # Convert from onehot\n",
    "#     targets_test_int = [np.where(r == 1)[0][0] for r in targets_test_hosp]\n",
    "#     y_pred = current_model.predict(inputs_test_hosp)\n",
    "#     y_pred2 = np.argmax(y_pred, axis = 1)\n",
    "#     # Test accuracy:\n",
    "#     acc = accuracy_score(targets_test_int, y_pred2)\n",
    "#     print('Accuracy in this domain = ', acc)\n",
    "#     # Confusion matrix for target\n",
    "#     conf_mat = confusion_matrix(targets_test_int, y_pred2)\n",
    "#     print(conf_mat)\n",
    "#     # Per class metrics\n",
    "#     class_report = classification_report(targets_test_int, y_pred2, target_names=target_names)\n",
    "#     print(class_report)\n",
    "    \n",
    "#     # Evaluate error on source data\n",
    "#     _, metric = current_model.evaluate(x=inputs_test_phys, y=targets_test_phys, batch_size=50)\n",
    "#     print('Accuracy on other domain = ', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
