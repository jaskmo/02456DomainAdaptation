{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import CSVLogger, Callback\n",
    "from datetime import datetime\n",
    "import keras.backend as K\n",
    "import extras.ourUtils as utils\n",
    "import numpy as np\n",
    "import Models\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from extras.ourUtils import reverse_data_split, create_data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nrEpochs = 10\n",
    "full_train = True\n",
    "path = 'C:/Users/DANN_maces_jaskmo/Documents/'\n",
    "source_data = path + 'newTaperImg/Physionet/'\n",
    "target_data = path + 'newTaperImg/Hospital/'\n",
    "stdout_cell = sys.stdout\n",
    "MIQ = ['DA', 'target', 'source']\n",
    "kf = KFold(n_splits = 10)\n",
    "n_subjects_phys = np.arange(1,21);\n",
    "n_subjects_hosp = np.arange(1,35);\n",
    "\n",
    "class FlipControle(Callback):\n",
    "    def __init__(self, alphaIn):\n",
    "        self.alpha = alphaIn\n",
    "        print(K.get_value(lamFunk))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        p = (epoch+1)/nrEpochs\n",
    "        K.set_value(self.alpha, (2/(1+np.exp(-10*p)))-1)\n",
    "        print(K.get_value(lamFunk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data as generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# make a data generator for dplInput\n",
    "def train_gen_DAnet(source, target, batch_size):\n",
    "    half = batch_size//2\n",
    "    while True:\n",
    "        source_data, source_lable = source.next()\n",
    "        target_data, target_lable = target.next()\n",
    "        if len(source_lable) != batch_size or len(target_lable) != batch_size:\n",
    "            continue\n",
    "        dpl_data = np.concatenate((source_data[:half,...],target_data[:half,...]),axis=0)\n",
    "               \n",
    "        domain_tmp = np.ones(batch_size, dtype='int8')\n",
    "        domain_tmp[half:] = domain_tmp[half:] * 0\n",
    "        dpl_lable = np.concatenate((domain_tmp.reshape(batch_size,1),\n",
    "                                       np.flip(domain_tmp,0).reshape(batch_size,1)),1)\n",
    "\n",
    "        yield({'lplInput':source_data,'dplInput':dpl_data}, {'lplOut':source_lable,'dplOut':dpl_lable})\n",
    "        \n",
    "def test_gen_DAnet(source, target, batch_size):\n",
    "    half = batch_size//2\n",
    "    while True:\n",
    "        source_data, source_lable = source.next()\n",
    "        target_data, target_lable = target.next()\n",
    "        if len(source_lable) != batch_size or len(target_lable) != batch_size:\n",
    "            continue\n",
    "        dpl_data = np.concatenate((source_data[:half,...],target_data[:half,...]),axis=0)\n",
    "               \n",
    "        domain_tmp = np.ones(batch_size, dtype='int8')\n",
    "        domain_tmp[half:] = domain_tmp[half:] * 0\n",
    "        dpl_lable = np.concatenate((domain_tmp.reshape(batch_size,1),\n",
    "                                       np.flip(domain_tmp,0).reshape(batch_size,1)),1)\n",
    "\n",
    "        yield({'lplInput':target_data,'dplInput':dpl_data}, {'lplOut':target_lable,'dplOut':dpl_lable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = dir()\n",
    "whitelist.append('whitelist')\n",
    "whitelist.append('phys_split')\n",
    "whitelist.append('hosp_split')\n",
    "whitelist.append('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22053 images belonging to 5 classes.\n",
      "Found 19037 images belonging to 5 classes.\n",
      "Found 3485 images belonging to 5 classes.\n",
      "Found 3675 images belonging to 5 classes.\n",
      "Found 2816 images belonging to 5 classes.\n",
      "Found 2404 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n",
      "Epoch 1/10\n",
      "344/344 [==============================] - 175s 508ms/step - loss: 21.7243 - lplOut_loss: 1.0213 - dplOut_loss: 0.2821 - lplOut_categorical_accuracy: 0.6319 - dplOut_categorical_accuracy: 0.9073 - val_loss: 19.2586 - val_lplOut_loss: 0.7198 - val_dplOut_loss: 0.6329 - val_lplOut_categorical_accuracy: 0.7358 - val_dplOut_categorical_accuracy: 0.7135\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "344/344 [==============================] - 169s 491ms/step - loss: 16.9286 - lplOut_loss: 0.6585 - dplOut_loss: 0.1387 - lplOut_categorical_accuracy: 0.7714 - dplOut_categorical_accuracy: 0.9549 - val_loss: 15.8623 - val_lplOut_loss: 0.6442 - val_dplOut_loss: 0.6134 - val_lplOut_categorical_accuracy: 0.7552 - val_dplOut_categorical_accuracy: 0.7248\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "344/344 [==============================] - 169s 490ms/step - loss: 14.1691 - lplOut_loss: 0.5728 - dplOut_loss: 0.1248 - lplOut_categorical_accuracy: 0.7980 - dplOut_categorical_accuracy: 0.9607 - val_loss: 13.7156 - val_lplOut_loss: 0.6207 - val_dplOut_loss: 0.6189 - val_lplOut_categorical_accuracy: 0.7636 - val_dplOut_categorical_accuracy: 0.7269\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "344/344 [==============================] - 169s 490ms/step - loss: 12.3688 - lplOut_loss: 0.5317 - dplOut_loss: 0.1343 - lplOut_categorical_accuracy: 0.8106 - dplOut_categorical_accuracy: 0.9599 - val_loss: 12.0952 - val_lplOut_loss: 0.5822 - val_dplOut_loss: 0.5059 - val_lplOut_categorical_accuracy: 0.7821 - val_dplOut_categorical_accuracy: 0.7384\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "344/344 [==============================] - 169s 491ms/step - loss: 11.2153 - lplOut_loss: 0.5025 - dplOut_loss: 0.2652 - lplOut_categorical_accuracy: 0.8187 - dplOut_categorical_accuracy: 0.9040 - val_loss: 11.1244 - val_lplOut_loss: 0.6240 - val_dplOut_loss: 0.5608 - val_lplOut_categorical_accuracy: 0.7772 - val_dplOut_categorical_accuracy: 0.7049\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "344/344 [==============================] - 169s 490ms/step - loss: 10.4306 - lplOut_loss: 0.4963 - dplOut_loss: 0.4226 - lplOut_categorical_accuracy: 0.8215 - dplOut_categorical_accuracy: 0.8216 - val_loss: 10.3116 - val_lplOut_loss: 0.5993 - val_dplOut_loss: 0.6056 - val_lplOut_categorical_accuracy: 0.7784 - val_dplOut_categorical_accuracy: 0.6670\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "344/344 [==============================] - 169s 491ms/step - loss: 9.7121 - lplOut_loss: 0.4819 - dplOut_loss: 0.4877 - lplOut_categorical_accuracy: 0.8252 - dplOut_categorical_accuracy: 0.7954 - val_loss: 9.5524 - val_lplOut_loss: 0.5811 - val_dplOut_loss: 0.5801 - val_lplOut_categorical_accuracy: 0.7894 - val_dplOut_categorical_accuracy: 0.7092\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "344/344 [==============================] - 169s 491ms/step - loss: 9.0815 - lplOut_loss: 0.4689 - dplOut_loss: 0.5470 - lplOut_categorical_accuracy: 0.8290 - dplOut_categorical_accuracy: 0.7494 - val_loss: 8.9405 - val_lplOut_loss: 0.5941 - val_dplOut_loss: 0.5972 - val_lplOut_categorical_accuracy: 0.7792 - val_dplOut_categorical_accuracy: 0.6612\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "344/344 [==============================] - 169s 492ms/step - loss: 8.5019 - lplOut_loss: 0.4518 - dplOut_loss: 0.5973 - lplOut_categorical_accuracy: 0.8344 - dplOut_categorical_accuracy: 0.6926 - val_loss: 8.3533 - val_lplOut_loss: 0.6071 - val_dplOut_loss: 0.5835 - val_lplOut_categorical_accuracy: 0.7812 - val_dplOut_categorical_accuracy: 0.7057\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "344/344 [==============================] - 169s 492ms/step - loss: 7.9049 - lplOut_loss: 0.4402 - dplOut_loss: 0.5739 - lplOut_categorical_accuracy: 0.8389 - dplOut_categorical_accuracy: 0.7380 - val_loss: 7.7941 - val_lplOut_loss: 0.5796 - val_dplOut_loss: 0.5899 - val_lplOut_categorical_accuracy: 0.7760 - val_dplOut_categorical_accuracy: 0.7321\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "297/297 [==============================] - 76s 254ms/step - loss: 12.1236 - categorical_accuracy: 0.5788 - val_loss: 11.4735 - val_categorical_accuracy: 0.6012\n",
      "Epoch 2/10\n",
      "297/297 [==============================] - 75s 251ms/step - loss: 10.7433 - categorical_accuracy: 0.7234 - val_loss: 10.5046 - val_categorical_accuracy: 0.6283\n",
      "Epoch 3/10\n",
      "297/297 [==============================] - 75s 252ms/step - loss: 9.8778 - categorical_accuracy: 0.7511 - val_loss: 9.7670 - val_categorical_accuracy: 0.6483\n",
      "Epoch 4/10\n",
      "297/297 [==============================] - 74s 251ms/step - loss: 9.1992 - categorical_accuracy: 0.7698 - val_loss: 9.1567 - val_categorical_accuracy: 0.6675\n",
      "Epoch 5/10\n",
      "297/297 [==============================] - 74s 251ms/step - loss: 8.6317 - categorical_accuracy: 0.7828 - val_loss: 8.6435 - val_categorical_accuracy: 0.6702\n",
      "Epoch 6/10\n",
      "297/297 [==============================] - 75s 251ms/step - loss: 8.1354 - categorical_accuracy: 0.7953 - val_loss: 8.1827 - val_categorical_accuracy: 0.6867\n",
      "Epoch 7/10\n",
      "297/297 [==============================] - 75s 253ms/step - loss: 7.6942 - categorical_accuracy: 0.8062 - val_loss: 7.7923 - val_categorical_accuracy: 0.6817\n",
      "Epoch 8/10\n",
      "297/297 [==============================] - 74s 250ms/step - loss: 7.2939 - categorical_accuracy: 0.8150 - val_loss: 7.4364 - val_categorical_accuracy: 0.6875\n",
      "Epoch 9/10\n",
      "297/297 [==============================] - 75s 251ms/step - loss: 6.9283 - categorical_accuracy: 0.8207 - val_loss: 7.0889 - val_categorical_accuracy: 0.6919\n",
      "Epoch 10/10\n",
      "297/297 [==============================] - 74s 251ms/step - loss: 6.5838 - categorical_accuracy: 0.8316 - val_loss: 6.7868 - val_categorical_accuracy: 0.6944\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "344/344 [==============================] - 85s 246ms/step - loss: 11.7435 - categorical_accuracy: 0.6686 - val_loss: 10.7817 - val_categorical_accuracy: 0.7422\n",
      "Epoch 2/10\n",
      "344/344 [==============================] - 85s 246ms/step - loss: 10.1504 - categorical_accuracy: 0.7817 - val_loss: 9.6646 - val_categorical_accuracy: 0.7584\n",
      "Epoch 3/10\n",
      "344/344 [==============================] - 85s 246ms/step - loss: 9.1722 - categorical_accuracy: 0.7993 - val_loss: 8.8591 - val_categorical_accuracy: 0.7590\n",
      "Epoch 4/10\n",
      "344/344 [==============================] - 85s 248ms/step - loss: 8.4315 - categorical_accuracy: 0.8156 - val_loss: 8.2239 - val_categorical_accuracy: 0.7616\n",
      "Epoch 5/10\n",
      "344/344 [==============================] - 84s 244ms/step - loss: 7.8272 - categorical_accuracy: 0.8222 - val_loss: 7.6910 - val_categorical_accuracy: 0.7697\n",
      "Epoch 6/10\n",
      "344/344 [==============================] - 85s 246ms/step - loss: 7.3102 - categorical_accuracy: 0.8340 - val_loss: 7.2313 - val_categorical_accuracy: 0.7656\n",
      "Epoch 7/10\n",
      "344/344 [==============================] - 85s 248ms/step - loss: 6.8573 - categorical_accuracy: 0.8388 - val_loss: 6.8074 - val_categorical_accuracy: 0.7688\n",
      "Epoch 8/10\n",
      "344/344 [==============================] - 85s 248ms/step - loss: 6.4383 - categorical_accuracy: 0.8512 - val_loss: 6.4328 - val_categorical_accuracy: 0.7705\n",
      "Epoch 9/10\n",
      "344/344 [==============================] - 85s 248ms/step - loss: 6.0599 - categorical_accuracy: 0.8571 - val_loss: 6.1002 - val_categorical_accuracy: 0.7708\n",
      "Epoch 10/10\n",
      "344/344 [==============================] - 85s 247ms/step - loss: 5.7112 - categorical_accuracy: 0.8667 - val_loss: 5.7851 - val_categorical_accuracy: 0.7653\n",
      "Found 23187 images belonging to 5 classes.\n",
      "Found 18801 images belonging to 5 classes.\n",
      "Found 2646 images belonging to 5 classes.\n",
      "Found 2976 images belonging to 5 classes.\n",
      "Found 2489 images belonging to 5 classes.\n",
      "Found 3339 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 167s 461ms/step - loss: 21.6230 - lplOut_loss: 0.9916 - dplOut_loss: 0.3128 - lplOut_categorical_accuracy: 0.6283 - dplOut_categorical_accuracy: 0.8870 - val_loss: 18.4963 - val_lplOut_loss: 0.5606 - val_dplOut_loss: 0.2025 - val_lplOut_categorical_accuracy: 0.7957 - val_dplOut_categorical_accuracy: 0.9310\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 166s 458ms/step - loss: 16.7827 - lplOut_loss: 0.6472 - dplOut_loss: 0.1883 - lplOut_categorical_accuracy: 0.7753 - dplOut_categorical_accuracy: 0.9312 - val_loss: 15.1272 - val_lplOut_loss: 0.5343 - val_dplOut_loss: 0.1701 - val_lplOut_categorical_accuracy: 0.8136 - val_dplOut_categorical_accuracy: 0.9428\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 166s 460ms/step - loss: 14.0440 - lplOut_loss: 0.5683 - dplOut_loss: 0.1696 - lplOut_categorical_accuracy: 0.7963 - dplOut_categorical_accuracy: 0.9418 - val_loss: 13.0082 - val_lplOut_loss: 0.5153 - val_dplOut_loss: 0.1631 - val_lplOut_categorical_accuracy: 0.8190 - val_dplOut_categorical_accuracy: 0.9512\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 166s 458ms/step - loss: 12.2911 - lplOut_loss: 0.5279 - dplOut_loss: 0.1864 - lplOut_categorical_accuracy: 0.8087 - dplOut_categorical_accuracy: 0.9353 - val_loss: 11.5970 - val_lplOut_loss: 0.4770 - val_dplOut_loss: 0.2192 - val_lplOut_categorical_accuracy: 0.8354 - val_dplOut_categorical_accuracy: 0.9310\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 166s 457ms/step - loss: 11.1641 - lplOut_loss: 0.5004 - dplOut_loss: 0.3082 - lplOut_categorical_accuracy: 0.8175 - dplOut_categorical_accuracy: 0.8783 - val_loss: 10.7261 - val_lplOut_loss: 0.4726 - val_dplOut_loss: 0.3962 - val_lplOut_categorical_accuracy: 0.8296 - val_dplOut_categorical_accuracy: 0.8319\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 166s 458ms/step - loss: 10.3326 - lplOut_loss: 0.4952 - dplOut_loss: 0.4071 - lplOut_categorical_accuracy: 0.8188 - dplOut_categorical_accuracy: 0.8263 - val_loss: 9.9281 - val_lplOut_loss: 0.4607 - val_dplOut_loss: 0.4423 - val_lplOut_categorical_accuracy: 0.8304 - val_dplOut_categorical_accuracy: 0.8201\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 166s 459ms/step - loss: 9.6424 - lplOut_loss: 0.4865 - dplOut_loss: 0.4940 - lplOut_categorical_accuracy: 0.8214 - dplOut_categorical_accuracy: 0.7792 - val_loss: 9.2920 - val_lplOut_loss: 0.4703 - val_dplOut_loss: 0.5105 - val_lplOut_categorical_accuracy: 0.8308 - val_dplOut_categorical_accuracy: 0.7896\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 166s 459ms/step - loss: 8.9754 - lplOut_loss: 0.4658 - dplOut_loss: 0.5236 - lplOut_categorical_accuracy: 0.8297 - dplOut_categorical_accuracy: 0.7744 - val_loss: 8.7130 - val_lplOut_loss: 0.4475 - val_dplOut_loss: 0.5961 - val_lplOut_categorical_accuracy: 0.8361 - val_dplOut_categorical_accuracy: 0.7027\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 165s 455ms/step - loss: 8.3915 - lplOut_loss: 0.4588 - dplOut_loss: 0.5617 - lplOut_categorical_accuracy: 0.8304 - dplOut_categorical_accuracy: 0.7277 - val_loss: 8.0877 - val_lplOut_loss: 0.4383 - val_dplOut_loss: 0.5694 - val_lplOut_categorical_accuracy: 0.8396 - val_dplOut_categorical_accuracy: 0.7317\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 164s 454ms/step - loss: 7.8184 - lplOut_loss: 0.4393 - dplOut_loss: 0.5723 - lplOut_categorical_accuracy: 0.8379 - dplOut_categorical_accuracy: 0.7236 - val_loss: 7.5262 - val_lplOut_loss: 0.4529 - val_dplOut_loss: 0.5336 - val_lplOut_categorical_accuracy: 0.8327 - val_dplOut_categorical_accuracy: 0.7923\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 73s 248ms/step - loss: 12.1785 - categorical_accuracy: 0.5779 - val_loss: 11.3055 - val_categorical_accuracy: 0.7232\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 70s 239ms/step - loss: 10.8193 - categorical_accuracy: 0.7252 - val_loss: 10.3649 - val_categorical_accuracy: 0.7408\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 72s 246ms/step - loss: 9.9637 - categorical_accuracy: 0.7618 - val_loss: 9.6594 - val_categorical_accuracy: 0.7412\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 73s 248ms/step - loss: 9.2770 - categorical_accuracy: 0.7780 - val_loss: 9.0718 - val_categorical_accuracy: 0.7469\n",
      "Epoch 5/10\n",
      "293/293 [==============================] - 72s 247ms/step - loss: 8.7102 - categorical_accuracy: 0.7929 - val_loss: 8.5705 - val_categorical_accuracy: 0.7442\n",
      "Epoch 6/10\n",
      "293/293 [==============================] - 72s 246ms/step - loss: 8.2006 - categorical_accuracy: 0.8102 - val_loss: 8.1237 - val_categorical_accuracy: 0.7432\n",
      "Epoch 7/10\n",
      "293/293 [==============================] - 73s 248ms/step - loss: 7.7564 - categorical_accuracy: 0.8165 - val_loss: 7.7246 - val_categorical_accuracy: 0.7435\n",
      "Epoch 8/10\n",
      "293/293 [==============================] - 73s 249ms/step - loss: 7.3403 - categorical_accuracy: 0.8282 - val_loss: 7.3547 - val_categorical_accuracy: 0.7466\n",
      "Epoch 9/10\n",
      "293/293 [==============================] - 73s 248ms/step - loss: 6.9636 - categorical_accuracy: 0.8346 - val_loss: 7.0138 - val_categorical_accuracy: 0.7446\n",
      "Epoch 10/10\n",
      "293/293 [==============================] - 73s 248ms/step - loss: 6.6196 - categorical_accuracy: 0.8402 - val_loss: 6.6892 - val_categorical_accuracy: 0.7469\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 85s 235ms/step - loss: 11.6998 - categorical_accuracy: 0.6674 - val_loss: 10.6250 - val_categorical_accuracy: 0.7969\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 84s 233ms/step - loss: 10.0942 - categorical_accuracy: 0.7727 - val_loss: 9.4879 - val_categorical_accuracy: 0.8163\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 86s 237ms/step - loss: 9.1050 - categorical_accuracy: 0.7944 - val_loss: 8.6633 - val_categorical_accuracy: 0.8216\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 86s 237ms/step - loss: 8.3617 - categorical_accuracy: 0.8081 - val_loss: 8.0153 - val_categorical_accuracy: 0.8300\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 86s 238ms/step - loss: 7.7578 - categorical_accuracy: 0.8240 - val_loss: 7.4844 - val_categorical_accuracy: 0.8331\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 85s 236ms/step - loss: 7.2370 - categorical_accuracy: 0.8334 - val_loss: 7.0077 - val_categorical_accuracy: 0.8373\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 86s 238ms/step - loss: 6.7780 - categorical_accuracy: 0.8418 - val_loss: 6.5783 - val_categorical_accuracy: 0.8430\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 86s 237ms/step - loss: 6.3587 - categorical_accuracy: 0.8476 - val_loss: 6.2011 - val_categorical_accuracy: 0.8399\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 86s 238ms/step - loss: 5.9797 - categorical_accuracy: 0.8545 - val_loss: 5.8468 - val_categorical_accuracy: 0.8426\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 86s 238ms/step - loss: 5.6294 - categorical_accuracy: 0.8628 - val_loss: 5.5279 - val_categorical_accuracy: 0.8426\n",
      "Found 22354 images belonging to 5 classes.\n",
      "Found 19066 images belonging to 5 classes.\n",
      "Found 2894 images belonging to 5 classes.\n",
      "Found 3105 images belonging to 5 classes.\n",
      "Found 3149 images belonging to 5 classes.\n",
      "Found 2957 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n",
      "Epoch 1/10\n",
      "349/349 [==============================] - 161s 462ms/step - loss: 21.5937 - lplOut_loss: 0.9532 - dplOut_loss: 0.2901 - lplOut_categorical_accuracy: 0.6504 - dplOut_categorical_accuracy: 0.9042 - val_loss: 18.6405 - val_lplOut_loss: 0.5851 - val_dplOut_loss: 0.2589 - val_lplOut_categorical_accuracy: 0.8066 - val_dplOut_categorical_accuracy: 0.9062\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 16.8157 - lplOut_loss: 0.6295 - dplOut_loss: 0.1680 - lplOut_categorical_accuracy: 0.7778 - dplOut_categorical_accuracy: 0.9410 - val_loss: 15.2566 - val_lplOut_loss: 0.5419 - val_dplOut_loss: 0.2169 - val_lplOut_categorical_accuracy: 0.8132 - val_dplOut_categorical_accuracy: 0.9233\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 14.0779 - lplOut_loss: 0.5532 - dplOut_loss: 0.1481 - lplOut_categorical_accuracy: 0.8011 - dplOut_categorical_accuracy: 0.9484 - val_loss: 13.1326 - val_lplOut_loss: 0.5229 - val_dplOut_loss: 0.2162 - val_lplOut_categorical_accuracy: 0.8194 - val_dplOut_categorical_accuracy: 0.9243\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 12.3068 - lplOut_loss: 0.5120 - dplOut_loss: 0.1611 - lplOut_categorical_accuracy: 0.8146 - dplOut_categorical_accuracy: 0.9465 - val_loss: 11.6882 - val_lplOut_loss: 0.4951 - val_dplOut_loss: 0.2416 - val_lplOut_categorical_accuracy: 0.8240 - val_dplOut_categorical_accuracy: 0.9139\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 11.1318 - lplOut_loss: 0.4801 - dplOut_loss: 0.2508 - lplOut_categorical_accuracy: 0.8266 - dplOut_categorical_accuracy: 0.9114 - val_loss: 10.7767 - val_lplOut_loss: 0.5279 - val_dplOut_loss: 0.3510 - val_lplOut_categorical_accuracy: 0.8024 - val_dplOut_categorical_accuracy: 0.8590\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 10.3336 - lplOut_loss: 0.4806 - dplOut_loss: 0.3787 - lplOut_categorical_accuracy: 0.8258 - dplOut_categorical_accuracy: 0.8480 - val_loss: 9.9517 - val_lplOut_loss: 0.4992 - val_dplOut_loss: 0.3788 - val_lplOut_categorical_accuracy: 0.8260 - val_dplOut_categorical_accuracy: 0.8528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950548\n",
      "Epoch 7/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 9.6104 - lplOut_loss: 0.4626 - dplOut_loss: 0.4355 - lplOut_categorical_accuracy: 0.8313 - dplOut_categorical_accuracy: 0.8223 - val_loss: 9.3708 - val_lplOut_loss: 0.5051 - val_dplOut_loss: 0.5019 - val_lplOut_categorical_accuracy: 0.8160 - val_dplOut_categorical_accuracy: 0.7764\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 8.9912 - lplOut_loss: 0.4573 - dplOut_loss: 0.4921 - lplOut_categorical_accuracy: 0.8332 - dplOut_categorical_accuracy: 0.8115 - val_loss: 8.7689 - val_lplOut_loss: 0.5175 - val_dplOut_loss: 0.5226 - val_lplOut_categorical_accuracy: 0.8139 - val_dplOut_categorical_accuracy: 0.7875\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 8.4211 - lplOut_loss: 0.4395 - dplOut_loss: 0.5464 - lplOut_categorical_accuracy: 0.8402 - dplOut_categorical_accuracy: 0.7536 - val_loss: 8.2145 - val_lplOut_loss: 0.5095 - val_dplOut_loss: 0.5568 - val_lplOut_categorical_accuracy: 0.8184 - val_dplOut_categorical_accuracy: 0.7361\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "349/349 [==============================] - 159s 457ms/step - loss: 7.8773 - lplOut_loss: 0.4228 - dplOut_loss: 0.5777 - lplOut_categorical_accuracy: 0.8450 - dplOut_categorical_accuracy: 0.7273 - val_loss: 7.6921 - val_lplOut_loss: 0.5372 - val_dplOut_loss: 0.5439 - val_lplOut_categorical_accuracy: 0.8052 - val_dplOut_categorical_accuracy: 0.7865\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "297/297 [==============================] - 73s 247ms/step - loss: 12.1231 - categorical_accuracy: 0.5781 - val_loss: 11.1553 - val_categorical_accuracy: 0.7793\n",
      "Epoch 2/10\n",
      "297/297 [==============================] - 74s 249ms/step - loss: 10.8072 - categorical_accuracy: 0.7180 - val_loss: 10.2467 - val_categorical_accuracy: 0.7897\n",
      "Epoch 3/10\n",
      "297/297 [==============================] - 74s 249ms/step - loss: 9.9617 - categorical_accuracy: 0.7485 - val_loss: 9.5525 - val_categorical_accuracy: 0.7946\n",
      "Epoch 4/10\n",
      "297/297 [==============================] - 74s 249ms/step - loss: 9.2872 - categorical_accuracy: 0.7720 - val_loss: 8.9836 - val_categorical_accuracy: 0.7926\n",
      "Epoch 5/10\n",
      "297/297 [==============================] - 72s 241ms/step - loss: 8.7310 - categorical_accuracy: 0.7823 - val_loss: 8.4868 - val_categorical_accuracy: 0.7952\n",
      "Epoch 6/10\n",
      "297/297 [==============================] - 73s 247ms/step - loss: 8.2363 - categorical_accuracy: 0.7953 - val_loss: 8.0574 - val_categorical_accuracy: 0.7926\n",
      "Epoch 7/10\n",
      "297/297 [==============================] - 74s 249ms/step - loss: 7.7970 - categorical_accuracy: 0.8077 - val_loss: 7.6634 - val_categorical_accuracy: 0.7939\n",
      "Epoch 8/10\n",
      "297/297 [==============================] - 74s 248ms/step - loss: 7.3957 - categorical_accuracy: 0.8128 - val_loss: 7.3048 - val_categorical_accuracy: 0.7936\n",
      "Epoch 9/10\n",
      "297/297 [==============================] - 73s 245ms/step - loss: 7.0252 - categorical_accuracy: 0.8250 - val_loss: 6.9720 - val_categorical_accuracy: 0.7962\n",
      "Epoch 10/10\n",
      "297/297 [==============================] - 73s 246ms/step - loss: 6.6827 - categorical_accuracy: 0.8344 - val_loss: 6.6558 - val_categorical_accuracy: 0.7992\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "349/349 [==============================] - 83s 238ms/step - loss: 11.8066 - categorical_accuracy: 0.6399 - val_loss: 10.6806 - val_categorical_accuracy: 0.7990\n",
      "Epoch 2/10\n",
      "349/349 [==============================] - 82s 234ms/step - loss: 10.1490 - categorical_accuracy: 0.7757 - val_loss: 9.5460 - val_categorical_accuracy: 0.8194\n",
      "Epoch 3/10\n",
      "349/349 [==============================] - 84s 240ms/step - loss: 9.1415 - categorical_accuracy: 0.8049 - val_loss: 8.7317 - val_categorical_accuracy: 0.8153\n",
      "Epoch 4/10\n",
      "349/349 [==============================] - 84s 240ms/step - loss: 8.3862 - categorical_accuracy: 0.8140 - val_loss: 8.1036 - val_categorical_accuracy: 0.8111\n",
      "Epoch 5/10\n",
      "349/349 [==============================] - 83s 239ms/step - loss: 7.7837 - categorical_accuracy: 0.8260 - val_loss: 7.5496 - val_categorical_accuracy: 0.8236\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 84s 241ms/step - loss: 7.2590 - categorical_accuracy: 0.8379 - val_loss: 7.0851 - val_categorical_accuracy: 0.8226\n",
      "Epoch 7/10\n",
      "349/349 [==============================] - 85s 243ms/step - loss: 6.7974 - categorical_accuracy: 0.8468 - val_loss: 6.6787 - val_categorical_accuracy: 0.8156\n",
      "Epoch 8/10\n",
      "349/349 [==============================] - 84s 242ms/step - loss: 6.3889 - categorical_accuracy: 0.8538 - val_loss: 6.3148 - val_categorical_accuracy: 0.8135\n",
      "Epoch 9/10\n",
      "349/349 [==============================] - 84s 242ms/step - loss: 6.0110 - categorical_accuracy: 0.8617 - val_loss: 5.9580 - val_categorical_accuracy: 0.8170\n",
      "Epoch 10/10\n",
      "349/349 [==============================] - 84s 242ms/step - loss: 5.6623 - categorical_accuracy: 0.8678 - val_loss: 5.6563 - val_categorical_accuracy: 0.8135\n",
      "Found 23214 images belonging to 5 classes.\n",
      "Found 19439 images belonging to 5 classes.\n",
      "Found 2519 images belonging to 5 classes.\n",
      "Found 3156 images belonging to 5 classes.\n",
      "Found 2589 images belonging to 5 classes.\n",
      "Found 2521 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n",
      "Epoch 1/10\n",
      "362/362 [==============================] - 165s 456ms/step - loss: 21.5659 - lplOut_loss: 0.9254 - dplOut_loss: 0.3139 - lplOut_categorical_accuracy: 0.6547 - dplOut_categorical_accuracy: 0.8854 - val_loss: 18.5657 - val_lplOut_loss: 0.6795 - val_dplOut_loss: 0.1247 - val_lplOut_categorical_accuracy: 0.7312 - val_dplOut_categorical_accuracy: 0.9675\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 165s 457ms/step - loss: 16.7962 - lplOut_loss: 0.6390 - dplOut_loss: 0.1769 - lplOut_categorical_accuracy: 0.7727 - dplOut_categorical_accuracy: 0.9371 - val_loss: 15.1889 - val_lplOut_loss: 0.6346 - val_dplOut_loss: 0.0942 - val_lplOut_categorical_accuracy: 0.7380 - val_dplOut_categorical_accuracy: 0.9760\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 166s 457ms/step - loss: 14.0741 - lplOut_loss: 0.5687 - dplOut_loss: 0.1627 - lplOut_categorical_accuracy: 0.7939 - dplOut_categorical_accuracy: 0.9438 - val_loss: 13.0841 - val_lplOut_loss: 0.6095 - val_dplOut_loss: 0.1109 - val_lplOut_categorical_accuracy: 0.7556 - val_dplOut_categorical_accuracy: 0.9663\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 166s 458ms/step - loss: 12.3304 - lplOut_loss: 0.5342 - dplOut_loss: 0.1882 - lplOut_categorical_accuracy: 0.8038 - dplOut_categorical_accuracy: 0.9370 - val_loss: 11.6606 - val_lplOut_loss: 0.5884 - val_dplOut_loss: 0.1418 - val_lplOut_categorical_accuracy: 0.7612 - val_dplOut_categorical_accuracy: 0.9696\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 166s 458ms/step - loss: 11.1959 - lplOut_loss: 0.5112 - dplOut_loss: 0.3021 - lplOut_categorical_accuracy: 0.8149 - dplOut_categorical_accuracy: 0.8788 - val_loss: 10.7736 - val_lplOut_loss: 0.5995 - val_dplOut_loss: 0.2928 - val_lplOut_categorical_accuracy: 0.7644 - val_dplOut_categorical_accuracy: 0.9018\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 166s 457ms/step - loss: 10.3804 - lplOut_loss: 0.5010 - dplOut_loss: 0.4239 - lplOut_categorical_accuracy: 0.8169 - dplOut_categorical_accuracy: 0.8134 - val_loss: 9.9947 - val_lplOut_loss: 0.5922 - val_dplOut_loss: 0.3509 - val_lplOut_categorical_accuracy: 0.7656 - val_dplOut_categorical_accuracy: 0.8798\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 165s 456ms/step - loss: 9.6553 - lplOut_loss: 0.4900 - dplOut_loss: 0.4775 - lplOut_categorical_accuracy: 0.8213 - dplOut_categorical_accuracy: 0.7901 - val_loss: 9.4140 - val_lplOut_loss: 0.5836 - val_dplOut_loss: 0.4927 - val_lplOut_categorical_accuracy: 0.7748 - val_dplOut_categorical_accuracy: 0.8157\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 165s 457ms/step - loss: 9.0081 - lplOut_loss: 0.4730 - dplOut_loss: 0.5214 - lplOut_categorical_accuracy: 0.8273 - dplOut_categorical_accuracy: 0.7758 - val_loss: 8.7785 - val_lplOut_loss: 0.5442 - val_dplOut_loss: 0.5372 - val_lplOut_categorical_accuracy: 0.7921 - val_dplOut_categorical_accuracy: 0.7608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993293\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 165s 456ms/step - loss: 8.4171 - lplOut_loss: 0.4559 - dplOut_loss: 0.5622 - lplOut_categorical_accuracy: 0.8323 - dplOut_categorical_accuracy: 0.7280 - val_loss: 8.2291 - val_lplOut_loss: 0.5850 - val_dplOut_loss: 0.5357 - val_lplOut_categorical_accuracy: 0.7624 - val_dplOut_categorical_accuracy: 0.8017\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 166s 458ms/step - loss: 7.8650 - lplOut_loss: 0.4479 - dplOut_loss: 0.5818 - lplOut_categorical_accuracy: 0.8371 - dplOut_categorical_accuracy: 0.7184 - val_loss: 7.7651 - val_lplOut_loss: 0.5404 - val_dplOut_loss: 0.6574 - val_lplOut_categorical_accuracy: 0.8033 - val_dplOut_categorical_accuracy: 0.6190\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "303/303 [==============================] - 75s 248ms/step - loss: 12.0849 - categorical_accuracy: 0.5805 - val_loss: 11.1866 - val_categorical_accuracy: 0.7079\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 75s 247ms/step - loss: 10.7318 - categorical_accuracy: 0.7209 - val_loss: 10.2521 - val_categorical_accuracy: 0.7350\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 74s 245ms/step - loss: 9.8653 - categorical_accuracy: 0.7583 - val_loss: 9.5356 - val_categorical_accuracy: 0.7449\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 75s 247ms/step - loss: 9.1872 - categorical_accuracy: 0.7761 - val_loss: 8.9534 - val_categorical_accuracy: 0.7452\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 75s 247ms/step - loss: 8.6190 - categorical_accuracy: 0.7892 - val_loss: 8.4485 - val_categorical_accuracy: 0.7532\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 75s 247ms/step - loss: 8.1101 - categorical_accuracy: 0.8059 - val_loss: 7.9961 - val_categorical_accuracy: 0.7561\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 75s 246ms/step - loss: 7.6675 - categorical_accuracy: 0.8121 - val_loss: 7.5886 - val_categorical_accuracy: 0.7583\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 74s 243ms/step - loss: 7.2565 - categorical_accuracy: 0.8218 - val_loss: 7.2181 - val_categorical_accuracy: 0.7573\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 74s 245ms/step - loss: 6.8793 - categorical_accuracy: 0.8300 - val_loss: 6.8624 - val_categorical_accuracy: 0.7647\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 75s 247ms/step - loss: 6.5221 - categorical_accuracy: 0.8402 - val_loss: 6.5446 - val_categorical_accuracy: 0.7669\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "362/362 [==============================] - 86s 237ms/step - loss: 11.7669 - categorical_accuracy: 0.6360 - val_loss: 10.7284 - val_categorical_accuracy: 0.7284\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 85s 236ms/step - loss: 10.1112 - categorical_accuracy: 0.7699 - val_loss: 9.5928 - val_categorical_accuracy: 0.7516\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 85s 236ms/step - loss: 9.1131 - categorical_accuracy: 0.7918 - val_loss: 8.7714 - val_categorical_accuracy: 0.7640\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 86s 238ms/step - loss: 8.3652 - categorical_accuracy: 0.8086 - val_loss: 8.1275 - val_categorical_accuracy: 0.7656\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 84s 233ms/step - loss: 7.7574 - categorical_accuracy: 0.8189 - val_loss: 7.5676 - val_categorical_accuracy: 0.7817\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 85s 236ms/step - loss: 7.2332 - categorical_accuracy: 0.8307 - val_loss: 7.0943 - val_categorical_accuracy: 0.7821\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 85s 235ms/step - loss: 6.7783 - categorical_accuracy: 0.8356 - val_loss: 6.6844 - val_categorical_accuracy: 0.7833\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 85s 234ms/step - loss: 6.3559 - categorical_accuracy: 0.8458 - val_loss: 6.2941 - val_categorical_accuracy: 0.7921\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 85s 236ms/step - loss: 5.9729 - categorical_accuracy: 0.8525 - val_loss: 5.9336 - val_categorical_accuracy: 0.7985\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/362 [==============================] - 86s 236ms/step - loss: 5.6224 - categorical_accuracy: 0.8612 - val_loss: 5.6301 - val_categorical_accuracy: 0.7953\n",
      "Found 21886 images belonging to 5 classes.\n",
      "Found 20843 images belonging to 5 classes.\n",
      "Found 3147 images belonging to 5 classes.\n",
      "Found 2112 images belonging to 5 classes.\n",
      "Found 3310 images belonging to 5 classes.\n",
      "Found 2161 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n",
      "Epoch 1/10\n",
      "341/341 [==============================] - 161s 472ms/step - loss: 21.7122 - lplOut_loss: 0.9531 - dplOut_loss: 0.2857 - lplOut_categorical_accuracy: 0.6348 - dplOut_categorical_accuracy: 0.9080 - val_loss: 18.8574 - val_lplOut_loss: 0.6795 - val_dplOut_loss: 0.1840 - val_lplOut_categorical_accuracy: 0.7455 - val_dplOut_categorical_accuracy: 0.9455\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "341/341 [==============================] - 161s 473ms/step - loss: 16.9752 - lplOut_loss: 0.6008 - dplOut_loss: 0.1391 - lplOut_categorical_accuracy: 0.7849 - dplOut_categorical_accuracy: 0.9556 - val_loss: 15.5222 - val_lplOut_loss: 0.6284 - val_dplOut_loss: 0.1752 - val_lplOut_categorical_accuracy: 0.7596 - val_dplOut_categorical_accuracy: 0.9359\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "341/341 [==============================] - 161s 471ms/step - loss: 14.2556 - lplOut_loss: 0.5432 - dplOut_loss: 0.1232 - lplOut_categorical_accuracy: 0.8064 - dplOut_categorical_accuracy: 0.9626 - val_loss: 13.3795 - val_lplOut_loss: 0.6116 - val_dplOut_loss: 0.1735 - val_lplOut_categorical_accuracy: 0.7669 - val_dplOut_categorical_accuracy: 0.9445\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "341/341 [==============================] - 160s 470ms/step - loss: 12.4525 - lplOut_loss: 0.4969 - dplOut_loss: 0.1345 - lplOut_categorical_accuracy: 0.8218 - dplOut_categorical_accuracy: 0.9588 - val_loss: 11.9294 - val_lplOut_loss: 0.5924 - val_dplOut_loss: 0.2113 - val_lplOut_categorical_accuracy: 0.7739 - val_dplOut_categorical_accuracy: 0.9302\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "341/341 [==============================] - 160s 470ms/step - loss: 11.2717 - lplOut_loss: 0.4713 - dplOut_loss: 0.2352 - lplOut_categorical_accuracy: 0.8287 - dplOut_categorical_accuracy: 0.9181 - val_loss: 11.0753 - val_lplOut_loss: 0.6114 - val_dplOut_loss: 0.4095 - val_lplOut_categorical_accuracy: 0.7618 - val_dplOut_categorical_accuracy: 0.8399\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "341/341 [==============================] - 160s 470ms/step - loss: 10.4928 - lplOut_loss: 0.4640 - dplOut_loss: 0.4028 - lplOut_categorical_accuracy: 0.8316 - dplOut_categorical_accuracy: 0.8299 - val_loss: 10.1747 - val_lplOut_loss: 0.5707 - val_dplOut_loss: 0.3823 - val_lplOut_categorical_accuracy: 0.7710 - val_dplOut_categorical_accuracy: 0.8769\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "341/341 [==============================] - 160s 470ms/step - loss: 9.8044 - lplOut_loss: 0.4568 - dplOut_loss: 0.4883 - lplOut_categorical_accuracy: 0.8349 - dplOut_categorical_accuracy: 0.7872 - val_loss: 9.6020 - val_lplOut_loss: 0.6047 - val_dplOut_loss: 0.4883 - val_lplOut_categorical_accuracy: 0.7682 - val_dplOut_categorical_accuracy: 0.8109\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "341/341 [==============================] - 160s 469ms/step - loss: 9.1697 - lplOut_loss: 0.4441 - dplOut_loss: 0.5399 - lplOut_categorical_accuracy: 0.8377 - dplOut_categorical_accuracy: 0.7513 - val_loss: 8.9904 - val_lplOut_loss: 0.5799 - val_dplOut_loss: 0.5397 - val_lplOut_categorical_accuracy: 0.7784 - val_dplOut_categorical_accuracy: 0.7761\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "341/341 [==============================] - 161s 471ms/step - loss: 8.5895 - lplOut_loss: 0.4299 - dplOut_loss: 0.5837 - lplOut_categorical_accuracy: 0.8412 - dplOut_categorical_accuracy: 0.7024 - val_loss: 8.4866 - val_lplOut_loss: 0.5896 - val_dplOut_loss: 0.6093 - val_lplOut_categorical_accuracy: 0.7698 - val_dplOut_categorical_accuracy: 0.6932\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "341/341 [==============================] - 160s 469ms/step - loss: 8.0097 - lplOut_loss: 0.4235 - dplOut_loss: 0.5695 - lplOut_categorical_accuracy: 0.8450 - dplOut_categorical_accuracy: 0.7403 - val_loss: 7.8924 - val_lplOut_loss: 0.5668 - val_dplOut_loss: 0.5747 - val_lplOut_categorical_accuracy: 0.7761 - val_dplOut_categorical_accuracy: 0.7443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "325/325 [==============================] - 77s 238ms/step - loss: 12.1114 - categorical_accuracy: 0.5727 - val_loss: 10.9836 - val_categorical_accuracy: 0.8097\n",
      "Epoch 2/10\n",
      "325/325 [==============================] - 77s 236ms/step - loss: 10.6817 - categorical_accuracy: 0.7243 - val_loss: 10.0162 - val_categorical_accuracy: 0.8087\n",
      "Epoch 3/10\n",
      "325/325 [==============================] - 75s 232ms/step - loss: 9.7818 - categorical_accuracy: 0.7540 - val_loss: 9.2798 - val_categorical_accuracy: 0.8106\n",
      "Epoch 4/10\n",
      "325/325 [==============================] - 76s 235ms/step - loss: 9.0797 - categorical_accuracy: 0.7756 - val_loss: 8.6651 - val_categorical_accuracy: 0.8187\n",
      "Epoch 5/10\n",
      "325/325 [==============================] - 76s 234ms/step - loss: 8.4808 - categorical_accuracy: 0.7894 - val_loss: 8.1358 - val_categorical_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "325/325 [==============================] - 77s 235ms/step - loss: 7.9626 - categorical_accuracy: 0.8034 - val_loss: 7.6785 - val_categorical_accuracy: 0.8191\n",
      "Epoch 7/10\n",
      "325/325 [==============================] - 77s 235ms/step - loss: 7.5031 - categorical_accuracy: 0.8120 - val_loss: 7.2572 - val_categorical_accuracy: 0.8201\n",
      "Epoch 8/10\n",
      "325/325 [==============================] - 76s 233ms/step - loss: 7.0774 - categorical_accuracy: 0.8257 - val_loss: 6.8583 - val_categorical_accuracy: 0.8300\n",
      "Epoch 9/10\n",
      "325/325 [==============================] - 76s 235ms/step - loss: 6.6873 - categorical_accuracy: 0.8316 - val_loss: 6.4929 - val_categorical_accuracy: 0.8381\n",
      "Epoch 10/10\n",
      "325/325 [==============================] - 77s 236ms/step - loss: 6.3286 - categorical_accuracy: 0.8373 - val_loss: 6.1681 - val_categorical_accuracy: 0.8348\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "341/341 [==============================] - 84s 246ms/step - loss: 11.7742 - categorical_accuracy: 0.6578 - val_loss: 10.7993 - val_categorical_accuracy: 0.7497\n",
      "Epoch 2/10\n",
      "341/341 [==============================] - 83s 243ms/step - loss: 10.1546 - categorical_accuracy: 0.7870 - val_loss: 9.6788 - val_categorical_accuracy: 0.7688\n",
      "Epoch 3/10\n",
      "341/341 [==============================] - 83s 244ms/step - loss: 9.1753 - categorical_accuracy: 0.8080 - val_loss: 8.8699 - val_categorical_accuracy: 0.7707\n",
      "Epoch 4/10\n",
      "341/341 [==============================] - 83s 242ms/step - loss: 8.4315 - categorical_accuracy: 0.8242 - val_loss: 8.2373 - val_categorical_accuracy: 0.7710\n",
      "Epoch 5/10\n",
      "341/341 [==============================] - 83s 244ms/step - loss: 7.8298 - categorical_accuracy: 0.8324 - val_loss: 7.6935 - val_categorical_accuracy: 0.7793\n",
      "Epoch 6/10\n",
      "341/341 [==============================] - 83s 244ms/step - loss: 7.3145 - categorical_accuracy: 0.8412 - val_loss: 7.2277 - val_categorical_accuracy: 0.7793\n",
      "Epoch 7/10\n",
      "341/341 [==============================] - 83s 244ms/step - loss: 6.8560 - categorical_accuracy: 0.8496 - val_loss: 6.8173 - val_categorical_accuracy: 0.7758\n",
      "Epoch 8/10\n",
      "341/341 [==============================] - 83s 244ms/step - loss: 6.4433 - categorical_accuracy: 0.8581 - val_loss: 6.4327 - val_categorical_accuracy: 0.7828\n",
      "Epoch 9/10\n",
      "341/341 [==============================] - 82s 240ms/step - loss: 6.0626 - categorical_accuracy: 0.8660 - val_loss: 6.0809 - val_categorical_accuracy: 0.7825\n",
      "Epoch 10/10\n",
      "341/341 [==============================] - 83s 243ms/step - loss: 5.7154 - categorical_accuracy: 0.8731 - val_loss: 5.7724 - val_categorical_accuracy: 0.7854\n",
      "Found 23295 images belonging to 5 classes.\n",
      "Found 20651 images belonging to 5 classes.\n",
      "Found 2869 images belonging to 5 classes.\n",
      "Found 1914 images belonging to 5 classes.\n",
      "Found 2311 images belonging to 5 classes.\n",
      "Found 2551 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 169s 465ms/step - loss: 21.5613 - lplOut_loss: 0.9641 - dplOut_loss: 0.2920 - lplOut_categorical_accuracy: 0.6483 - dplOut_categorical_accuracy: 0.8954 - val_loss: 18.4445 - val_lplOut_loss: 0.5201 - val_dplOut_loss: 0.2080 - val_lplOut_categorical_accuracy: 0.8200 - val_dplOut_categorical_accuracy: 0.9268\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 168s 463ms/step - loss: 16.7235 - lplOut_loss: 0.6332 - dplOut_loss: 0.1672 - lplOut_categorical_accuracy: 0.7748 - dplOut_categorical_accuracy: 0.9399 - val_loss: 15.0328 - val_lplOut_loss: 0.4562 - val_dplOut_loss: 0.1836 - val_lplOut_categorical_accuracy: 0.8395 - val_dplOut_categorical_accuracy: 0.9315\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 168s 464ms/step - loss: 13.9850 - lplOut_loss: 0.5642 - dplOut_loss: 0.1485 - lplOut_categorical_accuracy: 0.7975 - dplOut_categorical_accuracy: 0.9482 - val_loss: 12.9045 - val_lplOut_loss: 0.4299 - val_dplOut_loss: 0.1826 - val_lplOut_categorical_accuracy: 0.8466 - val_dplOut_categorical_accuracy: 0.9329\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 12.2087 - lplOut_loss: 0.5136 - dplOut_loss: 0.1601 - lplOut_categorical_accuracy: 0.8153 - dplOut_categorical_accuracy: 0.9450 - val_loss: 11.5033 - val_lplOut_loss: 0.4078 - val_dplOut_loss: 0.2401 - val_lplOut_categorical_accuracy: 0.8597 - val_dplOut_categorical_accuracy: 0.9109\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 167s 461ms/step - loss: 11.0480 - lplOut_loss: 0.4908 - dplOut_loss: 0.2517 - lplOut_categorical_accuracy: 0.8210 - dplOut_categorical_accuracy: 0.9048 - val_loss: 10.6494 - val_lplOut_loss: 0.4202 - val_dplOut_loss: 0.4267 - val_lplOut_categorical_accuracy: 0.8473 - val_dplOut_categorical_accuracy: 0.8036\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 10.2398 - lplOut_loss: 0.4825 - dplOut_loss: 0.3803 - lplOut_categorical_accuracy: 0.8243 - dplOut_categorical_accuracy: 0.8350 - val_loss: 9.8139 - val_lplOut_loss: 0.3892 - val_dplOut_loss: 0.4497 - val_lplOut_categorical_accuracy: 0.8626 - val_dplOut_categorical_accuracy: 0.8139\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 168s 461ms/step - loss: 9.4892 - lplOut_loss: 0.4719 - dplOut_loss: 0.4037 - lplOut_categorical_accuracy: 0.8267 - dplOut_categorical_accuracy: 0.8369 - val_loss: 9.2080 - val_lplOut_loss: 0.3976 - val_dplOut_loss: 0.5456 - val_lplOut_categorical_accuracy: 0.8576 - val_dplOut_categorical_accuracy: 0.7379\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 168s 463ms/step - loss: 8.9004 - lplOut_loss: 0.4566 - dplOut_loss: 0.5020 - lplOut_categorical_accuracy: 0.8320 - dplOut_categorical_accuracy: 0.7843 - val_loss: 8.5484 - val_lplOut_loss: 0.3944 - val_dplOut_loss: 0.5271 - val_lplOut_categorical_accuracy: 0.8615 - val_dplOut_categorical_accuracy: 0.7912\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 168s 464ms/step - loss: 8.3150 - lplOut_loss: 0.4481 - dplOut_loss: 0.5352 - lplOut_categorical_accuracy: 0.8333 - dplOut_categorical_accuracy: 0.7584 - val_loss: 8.0710 - val_lplOut_loss: 0.4090 - val_dplOut_loss: 0.6183 - val_lplOut_categorical_accuracy: 0.8590 - val_dplOut_categorical_accuracy: 0.6641\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 7.7776 - lplOut_loss: 0.4417 - dplOut_loss: 0.5633 - lplOut_categorical_accuracy: 0.8370 - dplOut_categorical_accuracy: 0.7356 - val_loss: 7.4276 - val_lplOut_loss: 0.3965 - val_dplOut_loss: 0.5240 - val_lplOut_categorical_accuracy: 0.8608 - val_dplOut_categorical_accuracy: 0.8129\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "322/322 [==============================] - 75s 233ms/step - loss: 12.0974 - categorical_accuracy: 0.5710 - val_loss: 10.9842 - val_categorical_accuracy: 0.8055\n",
      "Epoch 2/10\n",
      "322/322 [==============================] - 74s 231ms/step - loss: 10.6780 - categorical_accuracy: 0.7293 - val_loss: 10.0390 - val_categorical_accuracy: 0.8125\n",
      "Epoch 3/10\n",
      "322/322 [==============================] - 75s 234ms/step - loss: 9.7834 - categorical_accuracy: 0.7635 - val_loss: 9.3215 - val_categorical_accuracy: 0.8136\n",
      "Epoch 4/10\n",
      "322/322 [==============================] - 75s 232ms/step - loss: 9.0762 - categorical_accuracy: 0.7824 - val_loss: 8.7285 - val_categorical_accuracy: 0.8147\n",
      "Epoch 5/10\n",
      "322/322 [==============================] - 73s 228ms/step - loss: 8.4850 - categorical_accuracy: 0.7955 - val_loss: 8.1892 - val_categorical_accuracy: 0.8238\n",
      "Epoch 6/10\n",
      "322/322 [==============================] - 74s 230ms/step - loss: 7.9592 - categorical_accuracy: 0.8105 - val_loss: 7.7210 - val_categorical_accuracy: 0.8287\n",
      "Epoch 7/10\n",
      "322/322 [==============================] - 74s 229ms/step - loss: 7.5005 - categorical_accuracy: 0.8200 - val_loss: 7.3129 - val_categorical_accuracy: 0.8244\n",
      "Epoch 8/10\n",
      "322/322 [==============================] - 75s 233ms/step - loss: 7.0785 - categorical_accuracy: 0.8275 - val_loss: 6.9290 - val_categorical_accuracy: 0.8308\n",
      "Epoch 9/10\n",
      "322/322 [==============================] - 75s 233ms/step - loss: 6.6919 - categorical_accuracy: 0.8336 - val_loss: 6.5627 - val_categorical_accuracy: 0.8351\n",
      "Epoch 10/10\n",
      "322/322 [==============================] - 75s 232ms/step - loss: 6.3300 - categorical_accuracy: 0.8462 - val_loss: 6.2309 - val_categorical_accuracy: 0.8373\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 87s 241ms/step - loss: 11.7190 - categorical_accuracy: 0.6379 - val_loss: 10.4879 - val_categorical_accuracy: 0.8224\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 87s 241ms/step - loss: 10.0292 - categorical_accuracy: 0.7786 - val_loss: 9.3204 - val_categorical_accuracy: 0.8448\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 86s 236ms/step - loss: 9.0209 - categorical_accuracy: 0.8003 - val_loss: 8.4890 - val_categorical_accuracy: 0.8526\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 87s 240ms/step - loss: 8.2572 - categorical_accuracy: 0.8121 - val_loss: 7.8336 - val_categorical_accuracy: 0.8572\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 87s 240ms/step - loss: 7.6444 - categorical_accuracy: 0.8215 - val_loss: 7.2834 - val_categorical_accuracy: 0.8604\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 87s 241ms/step - loss: 7.1102 - categorical_accuracy: 0.8343 - val_loss: 6.8136 - val_categorical_accuracy: 0.8622\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 87s 241ms/step - loss: 6.6428 - categorical_accuracy: 0.8418 - val_loss: 6.3893 - val_categorical_accuracy: 0.8626\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 87s 241ms/step - loss: 6.2223 - categorical_accuracy: 0.8465 - val_loss: 6.0044 - val_categorical_accuracy: 0.8654\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 87s 240ms/step - loss: 5.8367 - categorical_accuracy: 0.8559 - val_loss: 5.6530 - val_categorical_accuracy: 0.8647\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 86s 236ms/step - loss: 5.4785 - categorical_accuracy: 0.8651 - val_loss: 5.3261 - val_categorical_accuracy: 0.8675\n",
      "Found 23281 images belonging to 5 classes.\n",
      "Found 20706 images belonging to 5 classes.\n",
      "Found 2868 images belonging to 5 classes.\n",
      "Found 2512 images belonging to 5 classes.\n",
      "Found 2173 images belonging to 5 classes.\n",
      "Found 1898 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n",
      "Epoch 1/10\n",
      "363/363 [==============================] - 169s 465ms/step - loss: 21.5429 - lplOut_loss: 0.9062 - dplOut_loss: 0.2986 - lplOut_categorical_accuracy: 0.6597 - dplOut_categorical_accuracy: 0.8936 - val_loss: 18.6115 - val_lplOut_loss: 0.4166 - val_dplOut_loss: 0.4390 - val_lplOut_categorical_accuracy: 0.8647 - val_dplOut_categorical_accuracy: 0.7759\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 16.7581 - lplOut_loss: 0.6302 - dplOut_loss: 0.1665 - lplOut_categorical_accuracy: 0.7709 - dplOut_categorical_accuracy: 0.9400 - val_loss: 15.2549 - val_lplOut_loss: 0.3634 - val_dplOut_loss: 0.4621 - val_lplOut_categorical_accuracy: 0.8668 - val_dplOut_categorical_accuracy: 0.7809\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 14.0049 - lplOut_loss: 0.5523 - dplOut_loss: 0.1486 - lplOut_categorical_accuracy: 0.7976 - dplOut_categorical_accuracy: 0.9498 - val_loss: 13.1010 - val_lplOut_loss: 0.3552 - val_dplOut_loss: 0.4272 - val_lplOut_categorical_accuracy: 0.8750 - val_dplOut_categorical_accuracy: 0.7876\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 12.2383 - lplOut_loss: 0.5164 - dplOut_loss: 0.1634 - lplOut_categorical_accuracy: 0.8097 - dplOut_categorical_accuracy: 0.9459 - val_loss: 11.6780 - val_lplOut_loss: 0.3477 - val_dplOut_loss: 0.4537 - val_lplOut_categorical_accuracy: 0.8707 - val_dplOut_categorical_accuracy: 0.7695\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 11.0866 - lplOut_loss: 0.4983 - dplOut_loss: 0.2600 - lplOut_categorical_accuracy: 0.8173 - dplOut_categorical_accuracy: 0.9050 - val_loss: 10.6982 - val_lplOut_loss: 0.3367 - val_dplOut_loss: 0.5350 - val_lplOut_categorical_accuracy: 0.8810 - val_dplOut_categorical_accuracy: 0.7148\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 167s 461ms/step - loss: 10.2963 - lplOut_loss: 0.4873 - dplOut_loss: 0.4093 - lplOut_categorical_accuracy: 0.8194 - dplOut_categorical_accuracy: 0.8260 - val_loss: 9.9042 - val_lplOut_loss: 0.3377 - val_dplOut_loss: 0.5709 - val_lplOut_categorical_accuracy: 0.8775 - val_dplOut_categorical_accuracy: 0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950548\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 9.5782 - lplOut_loss: 0.4775 - dplOut_loss: 0.4692 - lplOut_categorical_accuracy: 0.8244 - dplOut_categorical_accuracy: 0.8106 - val_loss: 9.2246 - val_lplOut_loss: 0.3452 - val_dplOut_loss: 0.6000 - val_lplOut_categorical_accuracy: 0.8750 - val_dplOut_categorical_accuracy: 0.6768\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 8.9562 - lplOut_loss: 0.4689 - dplOut_loss: 0.5358 - lplOut_categorical_accuracy: 0.8241 - dplOut_categorical_accuracy: 0.7669 - val_loss: 8.6411 - val_lplOut_loss: 0.3535 - val_dplOut_loss: 0.6566 - val_lplOut_categorical_accuracy: 0.8668 - val_dplOut_categorical_accuracy: 0.6271\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 167s 461ms/step - loss: 8.3574 - lplOut_loss: 0.4484 - dplOut_loss: 0.5777 - lplOut_categorical_accuracy: 0.8359 - dplOut_categorical_accuracy: 0.7240 - val_loss: 7.9868 - val_lplOut_loss: 0.3633 - val_dplOut_loss: 0.5858 - val_lplOut_categorical_accuracy: 0.8636 - val_dplOut_categorical_accuracy: 0.7269\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 168s 462ms/step - loss: 7.7781 - lplOut_loss: 0.4431 - dplOut_loss: 0.5738 - lplOut_categorical_accuracy: 0.8373 - dplOut_categorical_accuracy: 0.7273 - val_loss: 7.5219 - val_lplOut_loss: 0.3423 - val_dplOut_loss: 0.6893 - val_lplOut_categorical_accuracy: 0.8651 - val_dplOut_categorical_accuracy: 0.5859\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "323/323 [==============================] - 78s 240ms/step - loss: 11.9997 - categorical_accuracy: 0.6165 - val_loss: 11.3542 - val_categorical_accuracy: 0.5849\n",
      "Epoch 2/10\n",
      "323/323 [==============================] - 77s 238ms/step - loss: 10.5652 - categorical_accuracy: 0.7429 - val_loss: 10.4149 - val_categorical_accuracy: 0.5946\n",
      "Epoch 3/10\n",
      "323/323 [==============================] - 77s 238ms/step - loss: 9.6525 - categorical_accuracy: 0.7770 - val_loss: 9.6611 - val_categorical_accuracy: 0.6046\n",
      "Epoch 4/10\n",
      "323/323 [==============================] - 77s 238ms/step - loss: 8.9419 - categorical_accuracy: 0.7924 - val_loss: 9.0099 - val_categorical_accuracy: 0.6306\n",
      "Epoch 5/10\n",
      "323/323 [==============================] - 77s 238ms/step - loss: 8.3493 - categorical_accuracy: 0.8034 - val_loss: 8.4990 - val_categorical_accuracy: 0.6286\n",
      "Epoch 6/10\n",
      "323/323 [==============================] - 75s 233ms/step - loss: 7.8261 - categorical_accuracy: 0.8127 - val_loss: 8.0211 - val_categorical_accuracy: 0.6382\n",
      "Epoch 7/10\n",
      "323/323 [==============================] - 77s 238ms/step - loss: 7.3666 - categorical_accuracy: 0.8211 - val_loss: 7.5952 - val_categorical_accuracy: 0.6450\n",
      "Epoch 8/10\n",
      "323/323 [==============================] - 77s 238ms/step - loss: 6.9421 - categorical_accuracy: 0.8336 - val_loss: 7.1549 - val_categorical_accuracy: 0.6787\n",
      "Epoch 9/10\n",
      "323/323 [==============================] - 76s 235ms/step - loss: 6.5587 - categorical_accuracy: 0.8378 - val_loss: 6.8441 - val_categorical_accuracy: 0.6587\n",
      "Epoch 10/10\n",
      "323/323 [==============================] - 77s 238ms/step - loss: 6.2020 - categorical_accuracy: 0.8470 - val_loss: 6.5223 - val_categorical_accuracy: 0.6558\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "363/363 [==============================] - 86s 236ms/step - loss: 11.7206 - categorical_accuracy: 0.6469 - val_loss: 10.4579 - val_categorical_accuracy: 0.8590\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 86s 238ms/step - loss: 10.0985 - categorical_accuracy: 0.7715 - val_loss: 9.3321 - val_categorical_accuracy: 0.8725\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 87s 239ms/step - loss: 9.1108 - categorical_accuracy: 0.7928 - val_loss: 8.5197 - val_categorical_accuracy: 0.8789\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 86s 237ms/step - loss: 8.3618 - categorical_accuracy: 0.8102 - val_loss: 7.8920 - val_categorical_accuracy: 0.8693\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 87s 239ms/step - loss: 7.7572 - categorical_accuracy: 0.8223 - val_loss: 7.3581 - val_categorical_accuracy: 0.8679\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 86s 236ms/step - loss: 7.2314 - categorical_accuracy: 0.8324 - val_loss: 6.8855 - val_categorical_accuracy: 0.8732\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 86s 238ms/step - loss: 6.7709 - categorical_accuracy: 0.8404 - val_loss: 6.4893 - val_categorical_accuracy: 0.8636\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 86s 237ms/step - loss: 6.3598 - categorical_accuracy: 0.8461 - val_loss: 6.0978 - val_categorical_accuracy: 0.8679\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 87s 238ms/step - loss: 5.9675 - categorical_accuracy: 0.8589 - val_loss: 5.7602 - val_categorical_accuracy: 0.8636\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 87s 240ms/step - loss: 5.6182 - categorical_accuracy: 0.8632 - val_loss: 5.4354 - val_categorical_accuracy: 0.8675\n",
      "Found 22640 images belonging to 5 classes.\n",
      "Found 20281 images belonging to 5 classes.\n",
      "Found 2616 images belonging to 5 classes.\n",
      "Found 2263 images belonging to 5 classes.\n",
      "Found 3066 images belonging to 5 classes.\n",
      "Found 2747 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n",
      "Epoch 1/10\n",
      "353/353 [==============================] - 161s 457ms/step - loss: 21.6633 - lplOut_loss: 0.9745 - dplOut_loss: 0.3224 - lplOut_categorical_accuracy: 0.6253 - dplOut_categorical_accuracy: 0.8838 - val_loss: 18.7787 - val_lplOut_loss: 0.8010 - val_dplOut_loss: 0.1578 - val_lplOut_categorical_accuracy: 0.7438 - val_dplOut_categorical_accuracy: 0.9496\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 160s 454ms/step - loss: 16.8565 - lplOut_loss: 0.6336 - dplOut_loss: 0.1809 - lplOut_categorical_accuracy: 0.7695 - dplOut_categorical_accuracy: 0.9341 - val_loss: 15.4330 - val_lplOut_loss: 0.7676 - val_dplOut_loss: 0.1428 - val_lplOut_categorical_accuracy: 0.7539 - val_dplOut_categorical_accuracy: 0.9469\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 162s 458ms/step - loss: 14.1214 - lplOut_loss: 0.5565 - dplOut_loss: 0.1605 - lplOut_categorical_accuracy: 0.7959 - dplOut_categorical_accuracy: 0.9450 - val_loss: 13.2884 - val_lplOut_loss: 0.7340 - val_dplOut_loss: 0.1310 - val_lplOut_categorical_accuracy: 0.7602 - val_dplOut_categorical_accuracy: 0.9625\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 162s 458ms/step - loss: 12.3579 - lplOut_loss: 0.5166 - dplOut_loss: 0.1771 - lplOut_categorical_accuracy: 0.8124 - dplOut_categorical_accuracy: 0.9417 - val_loss: 11.8804 - val_lplOut_loss: 0.7364 - val_dplOut_loss: 0.1614 - val_lplOut_categorical_accuracy: 0.7656 - val_dplOut_categorical_accuracy: 0.9555\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 161s 457ms/step - loss: 11.2154 - lplOut_loss: 0.4987 - dplOut_loss: 0.2843 - lplOut_categorical_accuracy: 0.8156 - dplOut_categorical_accuracy: 0.8899 - val_loss: 10.8975 - val_lplOut_loss: 0.6808 - val_dplOut_loss: 0.2864 - val_lplOut_categorical_accuracy: 0.7820 - val_dplOut_categorical_accuracy: 0.9172\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 162s 458ms/step - loss: 10.4144 - lplOut_loss: 0.4870 - dplOut_loss: 0.4233 - lplOut_categorical_accuracy: 0.8209 - dplOut_categorical_accuracy: 0.8152 - val_loss: 10.1562 - val_lplOut_loss: 0.7271 - val_dplOut_loss: 0.3281 - val_lplOut_categorical_accuracy: 0.7688 - val_dplOut_categorical_accuracy: 0.8957\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 161s 457ms/step - loss: 9.6770 - lplOut_loss: 0.4737 - dplOut_loss: 0.4641 - lplOut_categorical_accuracy: 0.8236 - dplOut_categorical_accuracy: 0.7983 - val_loss: 9.5651 - val_lplOut_loss: 0.7412 - val_dplOut_loss: 0.4335 - val_lplOut_categorical_accuracy: 0.7609 - val_dplOut_categorical_accuracy: 0.8555\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 162s 458ms/step - loss: 9.0381 - lplOut_loss: 0.4575 - dplOut_loss: 0.5135 - lplOut_categorical_accuracy: 0.8311 - dplOut_categorical_accuracy: 0.7792 - val_loss: 8.9531 - val_lplOut_loss: 0.7466 - val_dplOut_loss: 0.4541 - val_lplOut_categorical_accuracy: 0.7633 - val_dplOut_categorical_accuracy: 0.8551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993293\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 161s 457ms/step - loss: 8.4626 - lplOut_loss: 0.4432 - dplOut_loss: 0.5616 - lplOut_categorical_accuracy: 0.8349 - dplOut_categorical_accuracy: 0.7278 - val_loss: 8.4277 - val_lplOut_loss: 0.7792 - val_dplOut_loss: 0.4784 - val_lplOut_categorical_accuracy: 0.7582 - val_dplOut_categorical_accuracy: 0.8324\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 161s 457ms/step - loss: 7.8880 - lplOut_loss: 0.4295 - dplOut_loss: 0.5587 - lplOut_categorical_accuracy: 0.8393 - dplOut_categorical_accuracy: 0.7451 - val_loss: 7.8351 - val_lplOut_loss: 0.6999 - val_dplOut_loss: 0.4998 - val_lplOut_categorical_accuracy: 0.7660 - val_dplOut_categorical_accuracy: 0.8422\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "316/316 [==============================] - 76s 240ms/step - loss: 12.1313 - categorical_accuracy: 0.5685 - val_loss: 11.1181 - val_categorical_accuracy: 0.7388\n",
      "Epoch 2/10\n",
      "316/316 [==============================] - 73s 232ms/step - loss: 10.6553 - categorical_accuracy: 0.7342 - val_loss: 10.2195 - val_categorical_accuracy: 0.7317\n",
      "Epoch 3/10\n",
      "316/316 [==============================] - 74s 234ms/step - loss: 9.7665 - categorical_accuracy: 0.7710 - val_loss: 9.5240 - val_categorical_accuracy: 0.7263\n",
      "Epoch 4/10\n",
      "316/316 [==============================] - 74s 235ms/step - loss: 9.0822 - categorical_accuracy: 0.7859 - val_loss: 8.9574 - val_categorical_accuracy: 0.7161\n",
      "Epoch 5/10\n",
      "316/316 [==============================] - 75s 236ms/step - loss: 8.4989 - categorical_accuracy: 0.8021 - val_loss: 8.4530 - val_categorical_accuracy: 0.7121\n",
      "Epoch 6/10\n",
      "316/316 [==============================] - 75s 236ms/step - loss: 7.9971 - categorical_accuracy: 0.8120 - val_loss: 8.0204 - val_categorical_accuracy: 0.7013\n",
      "Epoch 7/10\n",
      "316/316 [==============================] - 74s 236ms/step - loss: 7.5402 - categorical_accuracy: 0.8239 - val_loss: 7.6289 - val_categorical_accuracy: 0.6973\n",
      "Epoch 8/10\n",
      "316/316 [==============================] - 75s 236ms/step - loss: 7.1320 - categorical_accuracy: 0.8321 - val_loss: 7.2548 - val_categorical_accuracy: 0.7013\n",
      "Epoch 9/10\n",
      "316/316 [==============================] - 75s 237ms/step - loss: 6.7403 - categorical_accuracy: 0.8436 - val_loss: 6.9026 - val_categorical_accuracy: 0.7045\n",
      "Epoch 10/10\n",
      "316/316 [==============================] - 75s 238ms/step - loss: 6.3895 - categorical_accuracy: 0.8497 - val_loss: 6.6103 - val_categorical_accuracy: 0.6933\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "353/353 [==============================] - 84s 238ms/step - loss: 11.7465 - categorical_accuracy: 0.6430 - val_loss: 10.8281 - val_categorical_accuracy: 0.7375\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 84s 237ms/step - loss: 10.0945 - categorical_accuracy: 0.7741 - val_loss: 9.7116 - val_categorical_accuracy: 0.7504\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 83s 236ms/step - loss: 9.0858 - categorical_accuracy: 0.7991 - val_loss: 8.8925 - val_categorical_accuracy: 0.7586\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 83s 237ms/step - loss: 8.3368 - categorical_accuracy: 0.8122 - val_loss: 8.2403 - val_categorical_accuracy: 0.7633\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 84s 237ms/step - loss: 7.7237 - categorical_accuracy: 0.8237 - val_loss: 7.7080 - val_categorical_accuracy: 0.7617\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 83s 236ms/step - loss: 7.2039 - categorical_accuracy: 0.8331 - val_loss: 7.2551 - val_categorical_accuracy: 0.7688\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 83s 236ms/step - loss: 6.7424 - categorical_accuracy: 0.8412 - val_loss: 6.8513 - val_categorical_accuracy: 0.7660\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 84s 238ms/step - loss: 6.3254 - categorical_accuracy: 0.8485 - val_loss: 6.4511 - val_categorical_accuracy: 0.7723\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 83s 235ms/step - loss: 5.9456 - categorical_accuracy: 0.8597 - val_loss: 6.1264 - val_categorical_accuracy: 0.7625\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 84s 237ms/step - loss: 5.5927 - categorical_accuracy: 0.8650 - val_loss: 5.8180 - val_categorical_accuracy: 0.7668\n",
      "Found 21643 images belonging to 5 classes.\n",
      "Found 20163 images belonging to 5 classes.\n",
      "Found 3637 images belonging to 5 classes.\n",
      "Found 2570 images belonging to 5 classes.\n",
      "Found 3042 images belonging to 5 classes.\n",
      "Found 2383 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n",
      "Epoch 1/10\n",
      "338/338 [==============================] - 163s 482ms/step - loss: 21.8121 - lplOut_loss: 1.0103 - dplOut_loss: 0.3148 - lplOut_categorical_accuracy: 0.6182 - dplOut_categorical_accuracy: 0.8861 - val_loss: 18.8319 - val_lplOut_loss: 0.5720 - val_dplOut_loss: 0.2301 - val_lplOut_categorical_accuracy: 0.8097 - val_dplOut_categorical_accuracy: 0.9163\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "338/338 [==============================] - 161s 476ms/step - loss: 17.0835 - lplOut_loss: 0.6225 - dplOut_loss: 0.1736 - lplOut_categorical_accuracy: 0.7800 - dplOut_categorical_accuracy: 0.9384 - val_loss: 15.5204 - val_lplOut_loss: 0.5276 - val_dplOut_loss: 0.2098 - val_lplOut_categorical_accuracy: 0.8164 - val_dplOut_categorical_accuracy: 0.9216\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "338/338 [==============================] - 163s 481ms/step - loss: 14.3605 - lplOut_loss: 0.5439 - dplOut_loss: 0.1581 - lplOut_categorical_accuracy: 0.8025 - dplOut_categorical_accuracy: 0.9438 - val_loss: 13.3817 - val_lplOut_loss: 0.5196 - val_dplOut_loss: 0.1956 - val_lplOut_categorical_accuracy: 0.8086 - val_dplOut_categorical_accuracy: 0.9355\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "338/338 [==============================] - 162s 480ms/step - loss: 12.5726 - lplOut_loss: 0.5050 - dplOut_loss: 0.1747 - lplOut_categorical_accuracy: 0.8186 - dplOut_categorical_accuracy: 0.9401 - val_loss: 11.9233 - val_lplOut_loss: 0.5032 - val_dplOut_loss: 0.2241 - val_lplOut_categorical_accuracy: 0.8158 - val_dplOut_categorical_accuracy: 0.9244\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "338/338 [==============================] - 162s 480ms/step - loss: 11.3832 - lplOut_loss: 0.4742 - dplOut_loss: 0.2781 - lplOut_categorical_accuracy: 0.8284 - dplOut_categorical_accuracy: 0.8952 - val_loss: 10.9731 - val_lplOut_loss: 0.4914 - val_dplOut_loss: 0.3683 - val_lplOut_categorical_accuracy: 0.8298 - val_dplOut_categorical_accuracy: 0.8636\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "338/338 [==============================] - 162s 481ms/step - loss: 10.6172 - lplOut_loss: 0.4715 - dplOut_loss: 0.4677 - lplOut_categorical_accuracy: 0.8272 - dplOut_categorical_accuracy: 0.7866 - val_loss: 10.1877 - val_lplOut_loss: 0.5185 - val_dplOut_loss: 0.4025 - val_lplOut_categorical_accuracy: 0.8164 - val_dplOut_categorical_accuracy: 0.8602\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "338/338 [==============================] - 162s 480ms/step - loss: 9.8284 - lplOut_loss: 0.4550 - dplOut_loss: 0.4771 - lplOut_categorical_accuracy: 0.8347 - dplOut_categorical_accuracy: 0.7949 - val_loss: 9.6459 - val_lplOut_loss: 0.5177 - val_dplOut_loss: 0.5894 - val_lplOut_categorical_accuracy: 0.8156 - val_dplOut_categorical_accuracy: 0.7012\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "338/338 [==============================] - 162s 480ms/step - loss: 9.2069 - lplOut_loss: 0.4444 - dplOut_loss: 0.5551 - lplOut_categorical_accuracy: 0.8376 - dplOut_categorical_accuracy: 0.7484 - val_loss: 8.9531 - val_lplOut_loss: 0.5260 - val_dplOut_loss: 0.5430 - val_lplOut_categorical_accuracy: 0.8147 - val_dplOut_categorical_accuracy: 0.7826\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "338/338 [==============================] - 162s 480ms/step - loss: 8.5843 - lplOut_loss: 0.4288 - dplOut_loss: 0.5730 - lplOut_categorical_accuracy: 0.8433 - dplOut_categorical_accuracy: 0.7374 - val_loss: 8.3350 - val_lplOut_loss: 0.5249 - val_dplOut_loss: 0.5227 - val_lplOut_categorical_accuracy: 0.8092 - val_dplOut_categorical_accuracy: 0.7852\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "338/338 [==============================] - 162s 480ms/step - loss: 8.0001 - lplOut_loss: 0.4200 - dplOut_loss: 0.5697 - lplOut_categorical_accuracy: 0.8453 - dplOut_categorical_accuracy: 0.7460 - val_loss: 7.8670 - val_lplOut_loss: 0.5323 - val_dplOut_loss: 0.5954 - val_lplOut_categorical_accuracy: 0.8117 - val_dplOut_categorical_accuracy: 0.7054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "315/315 [==============================] - 76s 240ms/step - loss: 12.1363 - categorical_accuracy: 0.5482 - val_loss: 11.0421 - val_categorical_accuracy: 0.7691\n",
      "Epoch 2/10\n",
      "315/315 [==============================] - 75s 238ms/step - loss: 10.6641 - categorical_accuracy: 0.7278 - val_loss: 10.1202 - val_categorical_accuracy: 0.7746\n",
      "Epoch 3/10\n",
      "315/315 [==============================] - 76s 240ms/step - loss: 9.7773 - categorical_accuracy: 0.7640 - val_loss: 9.4039 - val_categorical_accuracy: 0.7766\n",
      "Epoch 4/10\n",
      "315/315 [==============================] - 76s 241ms/step - loss: 9.0890 - categorical_accuracy: 0.7786 - val_loss: 8.8175 - val_categorical_accuracy: 0.7836\n",
      "Epoch 5/10\n",
      "315/315 [==============================] - 75s 239ms/step - loss: 8.5148 - categorical_accuracy: 0.7914 - val_loss: 8.3183 - val_categorical_accuracy: 0.7801\n",
      "Epoch 6/10\n",
      "315/315 [==============================] - 75s 237ms/step - loss: 8.0211 - categorical_accuracy: 0.8021 - val_loss: 7.8860 - val_categorical_accuracy: 0.7801\n",
      "Epoch 7/10\n",
      "315/315 [==============================] - 75s 239ms/step - loss: 7.5898 - categorical_accuracy: 0.8095 - val_loss: 7.4889 - val_categorical_accuracy: 0.7777\n",
      "Epoch 8/10\n",
      "315/315 [==============================] - 75s 239ms/step - loss: 7.1855 - categorical_accuracy: 0.8215 - val_loss: 7.1447 - val_categorical_accuracy: 0.7754\n",
      "Epoch 9/10\n",
      "315/315 [==============================] - 75s 239ms/step - loss: 6.8180 - categorical_accuracy: 0.8267 - val_loss: 6.8015 - val_categorical_accuracy: 0.7777\n",
      "Epoch 10/10\n",
      "315/315 [==============================] - 75s 239ms/step - loss: 6.4779 - categorical_accuracy: 0.8369 - val_loss: 6.5166 - val_categorical_accuracy: 0.7707\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "338/338 [==============================] - 83s 247ms/step - loss: 11.7599 - categorical_accuracy: 0.6625 - val_loss: 10.6672 - val_categorical_accuracy: 0.8092\n",
      "Epoch 2/10\n",
      "338/338 [==============================] - 84s 247ms/step - loss: 10.1768 - categorical_accuracy: 0.7796 - val_loss: 9.5831 - val_categorical_accuracy: 0.8172\n",
      "Epoch 3/10\n",
      "338/338 [==============================] - 83s 246ms/step - loss: 9.1964 - categorical_accuracy: 0.8068 - val_loss: 8.7934 - val_categorical_accuracy: 0.8153\n",
      "Epoch 4/10\n",
      "338/338 [==============================] - 83s 246ms/step - loss: 8.4690 - categorical_accuracy: 0.8157 - val_loss: 8.1666 - val_categorical_accuracy: 0.8184\n",
      "Epoch 5/10\n",
      "338/338 [==============================] - 84s 248ms/step - loss: 7.8711 - categorical_accuracy: 0.8293 - val_loss: 7.6476 - val_categorical_accuracy: 0.8203\n",
      "Epoch 6/10\n",
      "338/338 [==============================] - 83s 246ms/step - loss: 7.3548 - categorical_accuracy: 0.8395 - val_loss: 7.2026 - val_categorical_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "338/338 [==============================] - 84s 250ms/step - loss: 6.9000 - categorical_accuracy: 0.8487 - val_loss: 6.7833 - val_categorical_accuracy: 0.8156\n",
      "Epoch 8/10\n",
      "338/338 [==============================] - 84s 248ms/step - loss: 6.4882 - categorical_accuracy: 0.8556 - val_loss: 6.4147 - val_categorical_accuracy: 0.8131\n",
      "Epoch 9/10\n",
      "338/338 [==============================] - 84s 248ms/step - loss: 6.1117 - categorical_accuracy: 0.8615 - val_loss: 6.0688 - val_categorical_accuracy: 0.8192\n",
      "Epoch 10/10\n",
      "338/338 [==============================] - 83s 246ms/step - loss: 5.7656 - categorical_accuracy: 0.8666 - val_loss: 5.7509 - val_categorical_accuracy: 0.8164\n",
      "Found 22133 images belonging to 5 classes.\n",
      "Found 20467 images belonging to 5 classes.\n",
      "Found 2684 images belonging to 5 classes.\n",
      "Found 2424 images belonging to 5 classes.\n",
      "Found 3505 images belonging to 5 classes.\n",
      "Found 2342 images belonging to 5 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 25088)        100352      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 512)          12845568    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 512)          262656      lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 512)          12845568    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 512)          0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 512)          262656      dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            2565        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            1026        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 41,039,175\n",
      "Trainable params: 28,632,071\n",
      "Non-trainable params: 12,407,104\n",
      "__________________________________________________________________________________________________\n",
      "0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "345/345 [==============================] - 161s 466ms/step - loss: 21.6148 - lplOut_loss: 0.9100 - dplOut_loss: 0.2782 - lplOut_categorical_accuracy: 0.6572 - dplOut_categorical_accuracy: 0.9043 - val_loss: 18.6768 - val_lplOut_loss: 0.5777 - val_dplOut_loss: 0.1762 - val_lplOut_categorical_accuracy: 0.7904 - val_dplOut_categorical_accuracy: 0.9390\n",
      "0.46211717\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 159s 461ms/step - loss: 16.9237 - lplOut_loss: 0.6080 - dplOut_loss: 0.1524 - lplOut_categorical_accuracy: 0.7807 - dplOut_categorical_accuracy: 0.9468 - val_loss: 15.4035 - val_lplOut_loss: 0.5474 - val_dplOut_loss: 0.2069 - val_lplOut_categorical_accuracy: 0.7992 - val_dplOut_categorical_accuracy: 0.9207\n",
      "0.7615942\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 159s 462ms/step - loss: 14.2050 - lplOut_loss: 0.5427 - dplOut_loss: 0.1366 - lplOut_categorical_accuracy: 0.8013 - dplOut_categorical_accuracy: 0.9545 - val_loss: 13.2232 - val_lplOut_loss: 0.5102 - val_dplOut_loss: 0.1753 - val_lplOut_categorical_accuracy: 0.8060 - val_dplOut_categorical_accuracy: 0.9379\n",
      "0.90514827\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 159s 461ms/step - loss: 12.4206 - lplOut_loss: 0.4988 - dplOut_loss: 0.1527 - lplOut_categorical_accuracy: 0.8202 - dplOut_categorical_accuracy: 0.9482 - val_loss: 11.7624 - val_lplOut_loss: 0.5078 - val_dplOut_loss: 0.1767 - val_lplOut_categorical_accuracy: 0.8087 - val_dplOut_categorical_accuracy: 0.9421\n",
      "0.9640276\n",
      "Epoch 5/10\n",
      "345/345 [==============================] - 159s 461ms/step - loss: 11.2279 - lplOut_loss: 0.4705 - dplOut_loss: 0.2401 - lplOut_categorical_accuracy: 0.8260 - dplOut_categorical_accuracy: 0.9103 - val_loss: 10.8720 - val_lplOut_loss: 0.5030 - val_dplOut_loss: 0.3661 - val_lplOut_categorical_accuracy: 0.8098 - val_dplOut_categorical_accuracy: 0.8396\n",
      "0.9866143\n",
      "Epoch 6/10\n",
      "345/345 [==============================] - 160s 463ms/step - loss: 10.4548 - lplOut_loss: 0.4679 - dplOut_loss: 0.4168 - lplOut_categorical_accuracy: 0.8288 - dplOut_categorical_accuracy: 0.8174 - val_loss: 10.0800 - val_lplOut_loss: 0.4880 - val_dplOut_loss: 0.4294 - val_lplOut_categorical_accuracy: 0.8041 - val_dplOut_categorical_accuracy: 0.8197\n",
      "0.9950548\n",
      "Epoch 7/10\n",
      "345/345 [==============================] - 159s 462ms/step - loss: 9.7211 - lplOut_loss: 0.4597 - dplOut_loss: 0.4642 - lplOut_categorical_accuracy: 0.8333 - dplOut_categorical_accuracy: 0.7979 - val_loss: 9.4863 - val_lplOut_loss: 0.5194 - val_dplOut_loss: 0.5216 - val_lplOut_categorical_accuracy: 0.8007 - val_dplOut_categorical_accuracy: 0.7721\n",
      "0.9981779\n",
      "Epoch 8/10\n",
      "345/345 [==============================] - 159s 462ms/step - loss: 9.1224 - lplOut_loss: 0.4483 - dplOut_loss: 0.5535 - lplOut_categorical_accuracy: 0.8357 - dplOut_categorical_accuracy: 0.7348 - val_loss: 8.8377 - val_lplOut_loss: 0.5003 - val_dplOut_loss: 0.5338 - val_lplOut_categorical_accuracy: 0.8148 - val_dplOut_categorical_accuracy: 0.7614\n",
      "0.9993293\n",
      "Epoch 9/10\n",
      "345/345 [==============================] - 159s 462ms/step - loss: 8.5068 - lplOut_loss: 0.4316 - dplOut_loss: 0.5685 - lplOut_categorical_accuracy: 0.8434 - dplOut_categorical_accuracy: 0.7313 - val_loss: 8.2869 - val_lplOut_loss: 0.4951 - val_dplOut_loss: 0.5758 - val_lplOut_categorical_accuracy: 0.8075 - val_dplOut_categorical_accuracy: 0.7355\n",
      "0.99975324\n",
      "Epoch 10/10\n",
      "345/345 [==============================] - 158s 458ms/step - loss: 7.9611 - lplOut_loss: 0.4270 - dplOut_loss: 0.5919 - lplOut_categorical_accuracy: 0.8427 - dplOut_categorical_accuracy: 0.7051 - val_loss: 7.7222 - val_lplOut_loss: 0.5573 - val_dplOut_loss: 0.4908 - val_lplOut_categorical_accuracy: 0.7923 - val_dplOut_categorical_accuracy: 0.8514\n",
      "0.9999092\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "319/319 [==============================] - 76s 240ms/step - loss: 12.0210 - categorical_accuracy: 0.6079 - val_loss: 11.1775 - val_categorical_accuracy: 0.7040\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 75s 236ms/step - loss: 10.6392 - categorical_accuracy: 0.7385 - val_loss: 10.2289 - val_categorical_accuracy: 0.7209\n",
      "Epoch 3/10\n",
      "319/319 [==============================] - 76s 237ms/step - loss: 9.7509 - categorical_accuracy: 0.7657 - val_loss: 9.4989 - val_categorical_accuracy: 0.7242\n",
      "Epoch 4/10\n",
      "319/319 [==============================] - 76s 238ms/step - loss: 9.0524 - categorical_accuracy: 0.7842 - val_loss: 8.8883 - val_categorical_accuracy: 0.7356\n",
      "Epoch 5/10\n",
      "319/319 [==============================] - 76s 238ms/step - loss: 8.4590 - categorical_accuracy: 0.7945 - val_loss: 8.3567 - val_categorical_accuracy: 0.7327\n",
      "Epoch 6/10\n",
      "319/319 [==============================] - 76s 237ms/step - loss: 7.9383 - categorical_accuracy: 0.8073 - val_loss: 7.9058 - val_categorical_accuracy: 0.7378\n",
      "Epoch 7/10\n",
      "319/319 [==============================] - 76s 238ms/step - loss: 7.4835 - categorical_accuracy: 0.8121 - val_loss: 7.4558 - val_categorical_accuracy: 0.7475\n",
      "Epoch 8/10\n",
      "319/319 [==============================] - 75s 234ms/step - loss: 7.0594 - categorical_accuracy: 0.8231 - val_loss: 7.0733 - val_categorical_accuracy: 0.7420\n",
      "Epoch 9/10\n",
      "319/319 [==============================] - 75s 235ms/step - loss: 6.6728 - categorical_accuracy: 0.8338 - val_loss: 6.7037 - val_categorical_accuracy: 0.7479\n",
      "Epoch 10/10\n",
      "319/319 [==============================] - 75s 236ms/step - loss: 6.3097 - categorical_accuracy: 0.8426 - val_loss: 6.3921 - val_categorical_accuracy: 0.7454\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "preLp (Flatten)              (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25088)             100352    \n",
      "_________________________________________________________________\n",
      "preDnsDo (Dropout)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "lpl1 (Dense)                 (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl1Do (Dropout)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lpl2 (Dense)                 (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "lplOut (Dense)               (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 27,929,925\n",
      "Trainable params: 15,522,821\n",
      "Non-trainable params: 12,407,104\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "345/345 [==============================] - 83s 241ms/step - loss: 11.7855 - categorical_accuracy: 0.6241 - val_loss: 10.6781 - val_categorical_accuracy: 0.7771\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 82s 238ms/step - loss: 10.1122 - categorical_accuracy: 0.7841 - val_loss: 9.5608 - val_categorical_accuracy: 0.7877\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 83s 239ms/step - loss: 9.1178 - categorical_accuracy: 0.8044 - val_loss: 8.7437 - val_categorical_accuracy: 0.7965\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 83s 240ms/step - loss: 8.3581 - categorical_accuracy: 0.8214 - val_loss: 8.0946 - val_categorical_accuracy: 0.7954\n",
      "Epoch 5/10\n",
      "345/345 [==============================] - 82s 239ms/step - loss: 7.7426 - categorical_accuracy: 0.8323 - val_loss: 7.5423 - val_categorical_accuracy: 0.7999\n",
      "Epoch 6/10\n",
      "345/345 [==============================] - 81s 235ms/step - loss: 7.2191 - categorical_accuracy: 0.8407 - val_loss: 7.0524 - val_categorical_accuracy: 0.8053\n",
      "Epoch 7/10\n",
      "345/345 [==============================] - 81s 236ms/step - loss: 6.7537 - categorical_accuracy: 0.8482 - val_loss: 6.6290 - val_categorical_accuracy: 0.8056\n",
      "Epoch 8/10\n",
      "345/345 [==============================] - 82s 238ms/step - loss: 6.3319 - categorical_accuracy: 0.8569 - val_loss: 6.2411 - val_categorical_accuracy: 0.8075\n",
      "Epoch 9/10\n",
      "345/345 [==============================] - 82s 237ms/step - loss: 5.9450 - categorical_accuracy: 0.8616 - val_loss: 5.8894 - val_categorical_accuracy: 0.8129\n",
      "Epoch 10/10\n",
      "345/345 [==============================] - 82s 238ms/step - loss: 5.5954 - categorical_accuracy: 0.8697 - val_loss: 5.5619 - val_categorical_accuracy: 0.8117\n"
     ]
    }
   ],
   "source": [
    "for phys_split, hosp_split in zip(kf.split(n_subjects_phys),kf.split(n_subjects_hosp)):\n",
    "\n",
    "    # memory management\n",
    "    this = sys.modules[__name__]\n",
    "    for n in dir():\n",
    "        if n not in whitelist: delattr(this, n)\n",
    "    K.clear_session()\n",
    "\n",
    "    #Define current time\n",
    "    now = datetime.now()\n",
    "    #print(\"PHYS:\\n\" + \"Train:\" + str(phys_split[0]) + '\\n' + 'Test:' + str(phys_split[1]) + \"\\n\")\n",
    "    #print(\"Hosp:\\n\" + \"Train:\" + str(hosp_split[0]) + '\\n' + 'Test:' + str(hosp_split[1]) + \"\\n\")\n",
    "    \n",
    "    reverse_data_split(source_data)\n",
    "    reverse_data_split(target_data)\n",
    "    \n",
    "    #Data split for physionet\n",
    "    create_data_split(source_data,phys_split[1]+1)\n",
    "    #Data split for target\n",
    "    create_data_split(target_data,hosp_split[1]+1)\n",
    "    \n",
    "    \n",
    "    #### Train data\n",
    "\n",
    "\n",
    "    train_gen_source = datagen.flow_from_directory(source_data + '/train', target_size=(224, 224), \n",
    "                                                   batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "    train_gen_target = datagen.flow_from_directory(target_data + '/train', target_size=(224, 224), \n",
    "                                                   batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "    train_gen_DA = train_gen_DAnet(train_gen_source, train_gen_target, batch_size)\n",
    "\n",
    "    train_stepE = np.floor_divide(train_gen_source.n, batch_size)\n",
    "    train_stepE_target = np.floor_divide(train_gen_target.n, batch_size)\n",
    "    #### validation data\n",
    "\n",
    "    valid_gen_source = datagen.flow_from_directory(source_data + '/validation', target_size=(224, 224), \n",
    "                                                   batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "    valid_gen_target = datagen.flow_from_directory(target_data + '/validation', target_size=(224, 224), \n",
    "                                                   batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "    #valid_gen_DA = test_gen_DAnet(valid_gen_source, valid_gen_target, batch_size)\n",
    "    valid_gen_DA = train_gen_DAnet(valid_gen_source, valid_gen_target, batch_size)\n",
    "    \n",
    "    valid_stepE = np.floor_divide(valid_gen_source.n, batch_size)\n",
    "    valid_stepE_target = np.floor_divide(valid_gen_target.n, batch_size)\n",
    "\n",
    "    #### test data\n",
    "\n",
    "    test_gen_source = datagen.flow_from_directory(source_data + '/test', target_size=(224, 224), \n",
    "                                                   batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "    test_gen_target = datagen.flow_from_directory(target_data + '/test', target_size=(224, 224), \n",
    "                                                   batch_size=batch_size, class_mode='categorical', shuffle=True)\n",
    "\n",
    "    test_gen_DA = test_gen_DAnet(test_gen_source, test_gen_target, batch_size)\n",
    "    \n",
    "\n",
    "    test_stepE = np.floor_divide(test_gen_source.n, batch_size)\n",
    "    test_stepE_target = np.floor_divide(test_gen_target.n, batch_size)   \n",
    "    \n",
    "    \n",
    "    #Define lamFunk\n",
    "    lamFunk = K.variable(0.0)\n",
    "    \n",
    "    for item in MIQ:\n",
    "        if item==\"DA\":         \n",
    "            current_model = Models.DA_model(lamFunk,do_rate_dpl=0, do_rate_lpl=0.5, vgg_train=False, nrUnits=[512,512])\n",
    "        else:\n",
    "            current_model = Models.lable_model(do_rate=0.5, vgg_train=False, nrUnits=[512,512])\n",
    "        \n",
    "        csv_logger = CSVLogger('C:/Users/DANN_maces_jaskmo/Documents/trainingLog/' + item + \n",
    "                        str(now.day) + '-' + str(now.month) + '-' + str(now.year) + '_' + \n",
    "                        str(now.hour) + str(now.minute) + '.log')\n",
    "        if item=='DA':\n",
    "            current_model.fit_generator(train_gen_DA, train_stepE, epochs=nrEpochs, verbose=1, validation_data=valid_gen_DA, \n",
    "                            validation_steps=valid_stepE, callbacks=[csv_logger,FlipControle(lamFunk)], initial_epoch=0,\n",
    "                            max_queue_size=5, class_weight = {'lplOut':[1,0.33,0.33,0.33,0.5],'dplOut':[1,1]})\n",
    "        elif item=='source':\n",
    "            current_model.fit_generator(train_gen_source, train_stepE, epochs=nrEpochs, verbose=1, validation_data=valid_gen_source, \n",
    "                            validation_steps=valid_stepE, callbacks=[csv_logger], initial_epoch=0,\n",
    "                            max_queue_size=5, class_weight = [1,0.33,0.33,0.33,0.5])\n",
    "        else:\n",
    "            current_model.fit_generator(train_gen_target, train_stepE_target, epochs=nrEpochs, verbose=1, validation_data=valid_gen_target, \n",
    "                            validation_steps=valid_stepE_target, callbacks=[csv_logger], initial_epoch=0,\n",
    "                            max_queue_size=5, class_weight = [1,0.33,0.33,0.33,0.5])\n",
    "                     \n",
    "        if item == \"DA\":\n",
    "            DAlpm = utils.dissect_DAlpm(current_model)\n",
    "            current_model = DAlpm\n",
    "            \n",
    "#         #Save the model\n",
    "#         current_model.save(filepath=path + 'models/'+ item + str(now.day) + '-' + str(now.month) + '-' + str(now.year) + '_' + \n",
    "#                        str(now.hour) + str(now.minute) + '.h5')\n",
    "        \n",
    "        #TEST\n",
    "        DTD_dic = {'Physionet':test_gen_source,'Hospital':test_gen_target}\n",
    "        # save to file \n",
    "        test_file = 'C:/Users/DANN_maces_jaskmo/Documents/testLog/'+ item + str(now.day) + '-' + str(now.month) + '-' + str(now.year) + '_' + str(now.hour) + str(now.minute) + '.log'\n",
    "        sys.stdout = open(test_file, 'w')\n",
    "        \n",
    "        for dom in ['Physionet', 'Hospital']:\n",
    "            test_img, test_lable = DTD_dic[dom].next()\n",
    "            for count in range(DTD_dic[dom].n//batch_size):\n",
    "                tmp_img, tmp_lable = DTD_dic[dom].next()\n",
    "                test_img = np.concatenate((test_img, tmp_img), axis=0)\n",
    "                test_lable = np.concatenate((test_lable, tmp_lable),axis=0)\n",
    "\n",
    "            # Compute the test metrecis \n",
    "            inv_map = {v: k for k, v in DTD_dic[dom].class_indices.items()}\n",
    "            target_names = list(inv_map.values())\n",
    "\n",
    "            targets_test_int = [np.where(r == 1)[0][0] for r in test_lable]\n",
    "            y_pred = current_model.predict(test_img)\n",
    "            y_pred2 = np.argmax(y_pred, axis = 1)\n",
    "            # Test accuracy:\n",
    "            acc = accuracy_score(targets_test_int, y_pred2)\n",
    "            \n",
    "            conf_mat = confusion_matrix(targets_test_int, y_pred2)\n",
    "            # Per class metrics\n",
    "            class_report = classification_report(targets_test_int, y_pred2, target_names=target_names)\n",
    "\n",
    "            print('Accuracy on ' + dom + ' data = ' + str(acc) +'\\n \\n' + \n",
    "                  'Confution matric on ' + dom + ' data: \\n' + str(conf_mat) + '\\n\\n' + \n",
    "                  'Class report on ' + dom + ' data: \\n' + class_report + '\\n\\n\\n\\n')\n",
    "\n",
    "        sys.stdout = stdout_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow env",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
