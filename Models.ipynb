{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Input\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy as acc\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.losses import categorical_crossentropy\n",
    "from extras.flip_gradient import flip_gradient\n",
    "from numpy import floor_divide\n",
    "import numpy as np\n",
    "#from ourUtils import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lable modle without DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lable_model(l2_reg = 0.01, do_rate = 0, vgg_train = True, nrUnits = [2048, 1024]):\n",
    "    # Load the convolutional part of the VGG16 network \n",
    "    vgg16Conv = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Input to network\n",
    "    vggInput = Input(shape=(224, 224, 3), name='image_input')\n",
    "    # Output of convolutional part\n",
    "    output_vgg16Conv = vgg16Conv(vggInput)\n",
    "    # Stack lable layers\n",
    "    preDns = Flatten(name='preLp')(output_vgg16Conv)\n",
    "    preDnsDo = Dropout(rate=do_rate, seed=42, name='preDnsDo')(preDns)\n",
    "    dns1 = Dense(nrUnits[0], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl1')(preDnsDo)\n",
    "    dns1Do = Dropout(rate=do_rate, seed=42, name='lpl1Do')(dns1)\n",
    "    dns2 = Dense(nrUnits[1], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl2')(dns1Do)\n",
    "    modelOut = Dense(5, activation='softmax', kernel_initializer='glorot_normal', name='lplOut')(dns2)\n",
    "\n",
    "    vggConvSleep = Model(inputs=vggInput, outputs=modelOut)\n",
    "\n",
    "    if not vgg_train:\n",
    "        for layer in vggConvSleep.layers[1].layers[:-2]:\n",
    "            layer.trainable = False\n",
    "        vggConvSleep.layers[1].layers[-2].kernel_regularizer = vggConvSleep.layers[-4].kernel_regularizer\n",
    "\n",
    "    # Optimizer\n",
    "    optimize = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    # Compile the model\n",
    "    vggConvSleep.compile(loss='categorical_crossentropy', optimizer=optimize, metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Get model summary\n",
    "    vggConvSleep.summary()\n",
    "    \n",
    "    return vggConvSleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DA_model(lamFunk, l2_reg = 0.01, do_rate_dpl = 0, do_rate_lpl = 0, vgg_train = True, nrUnits = [2048, 1024]):\n",
    "    \n",
    "    # Load the convolutional part of the VGG16 network \n",
    "    vgg16Conv = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Input to network\n",
    "    vggInput = Input(shape=(224, 224, 3), name='image_input')\n",
    "    # Output of convolutional part\n",
    "    output_vggConv = vgg16Conv(vggInput)\n",
    "    # pre Dence layer\n",
    "    preDns = Flatten(name='preDa')(output_vggConv)\n",
    "    # create the shared part as a model instance\n",
    "    sharedVGG16 = Model(inputs=vggInput, outputs=preDns)\n",
    "    \n",
    "    #Create lable predictive model\n",
    "    lpl_input = Input(shape=(224,224,3), name='lplInput')\n",
    "    # run lpl input through the shared part of the network\n",
    "    lpl_vgg_out = sharedVGG16(lpl_input)\n",
    "    lpl_vgg_outDo = Dropout(rate=do_rate_lpl, seed=42, name='lpl_vgg_outDo')(lpl_vgg_out)\n",
    "    lpl1 = Dense(nrUnits[0], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='lpl1')(lpl_vgg_outDo)\n",
    "    lpl1Do = Dropout(rate=do_rate_lpl, seed=42, name='lpl1Do')(lpl1)\n",
    "    lpl2 = Dense(nrUnits[1], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='lpl2')(lpl1Do)\n",
    "    lplOut = Dense(5, activation='softmax', kernel_initializer='glorot_normal', name='lplOut')(lpl2)\n",
    "    \n",
    "    #Create domain predictive model \n",
    "    dpl_input = Input(shape=(224,224,3), name='dplInput')\n",
    "    # run dpl input through the shared part of the network\n",
    "    dpl_vgg_out = sharedVGG16(dpl_input)\n",
    "    #lambdalayer for the flip gradient\n",
    "    flipGrad = Lambda(lambda x: flip_gradient(x,lamFunk),name='flipGrad')(dpl_vgg_out)\n",
    "    dpl1 = Dense(nrUnits[0], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='dpl1')(flipGrad)\n",
    "    dpl1Do = Dropout(rate=do_rate_dpl, seed=42, name='dpl1Do')(dpl1)\n",
    "    dpl2 = Dense(nrUnits[1], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='dpl2')(dpl1Do)\n",
    "    dplOut = Dense(2, activation='softmax', kernel_initializer='glorot_normal', name='dplOut')(dpl2)\n",
    "    \n",
    "    #stitch modle together\n",
    "    DAnetwork = Model(inputs=[lpl_input, dpl_input], outputs=[lplOut, dplOut]) \n",
    "    \n",
    "    if not vgg_train:\n",
    "        for layer in DAnetwork.layers[2].layers[1].layers[:-2]:\n",
    "            layer.trainable = False\n",
    "        DAnetwork.layers[2].layers[1].layers[-2].kernel_regularizer = DAnetwork.layers[-4].kernel_regularizer    \n",
    "    \n",
    "    # Optimizer\n",
    "    optimize = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    # Compile the model\n",
    "    DAnetwork.compile(optimizer=optimize, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Get model summary\n",
    "    DAnetwork.summary()\n",
    "    \n",
    "    return DAnetwork"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
