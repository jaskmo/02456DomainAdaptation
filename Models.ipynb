{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Input\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy as acc\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.losses import categorical_crossentropy\n",
    "from extras.flip_gradient import flip_gradient\n",
    "from keras.backend import in_test_phase, learning_phase\n",
    "from numpy import floor_divide\n",
    "import tensorflow as tf\n",
    "#from ourUtils import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lable modle without DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lable_model(l2_reg = 0.01, do_rate = 0, vgg_train = True):\n",
    "    # Load the convolutional part of the VGG16 network \n",
    "    vgg16Conv = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Input to network\n",
    "    vggInput = Input(shape=(224, 224, 3), name='image_input')\n",
    "    # Output of convolutional part\n",
    "    output_vgg16Conv = vgg16Conv(vggInput)\n",
    "    # Stack lable layers\n",
    "    preDns = Flatten(name='preLp')(output_vgg16Conv)\n",
    "    dns1 = Dense(2048, activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl1')(preDns)\n",
    "    dns1Do = Dropout(rate=do_rate, seed=42, name='lpl1Do')(dns1)\n",
    "    dns2 = Dense(1024, activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl2')(dns1Do)\n",
    "    modelOut = Dense(5, activation='softmax', kernel_initializer='glorot_normal', name='lplOut')(dns2)\n",
    "\n",
    "    vggConvSleep = Model(inputs=vggInput, outputs=modelOut)\n",
    "\n",
    "    if not vgg_train:\n",
    "        for layer in vggConvSleep.layers[:2]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # Optimizer\n",
    "    optimize = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    # Compile the model\n",
    "    vggConvSleep.compile(loss='categorical_crossentropy', optimizer=optimize, metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Get model summary\n",
    "    vggConvSleep.summary()\n",
    "    \n",
    "    return vggConvSleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DA_model(batch_size, l2_reg = 0.01, do_rate = 0, vgg_train = True):\n",
    "    #init\n",
    "    lam_slice = floor_divide(batch_size,2,dtype='int8')\n",
    "    # Load the convolutional part of the VGG16 network \n",
    "    vgg16Conv = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Input to network\n",
    "    vggInput = Input(shape=(224, 224, 3), name='image_input')\n",
    "    # Output of convolutional part\n",
    "    output_vggConv = vgg16Conv(vggInput)\n",
    "    # pre Dence layer\n",
    "    preDns = Flatten(name='preDa')(output_vggConv)\n",
    "    # Stack lable layers\n",
    "    #lplSlice = Lambda(lambda x: in_test_phase(x, x[:lam_slice, :]), name='lplSplit')(preDns)\n",
    "    lpl1 = Dense(2048, activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl1')(preDns) #(lplSlice)\n",
    "    lpl1Do = Dropout(rate=do_rate, seed=42, name='lpl1Do')(lpl1)\n",
    "    lpl2 = Dense(1024, activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl2')(lpl1Do)\n",
    "    lplOut = Dense(5, activation='softmax', kernel_initializer='glorot_normal', name='lplOut')(lpl2)\n",
    "    # Stack domain layers\n",
    "    flipGrad = Lambda(lambda x: flip_gradient(x,1),name='flipGrad')(preDns)\n",
    "    dpl1 = Dense(2048, activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='dpl1')(flipGrad)\n",
    "    dpl1Do = Dropout(rate=do_rate, seed=42, name='dpl1Do')(dpl1)\n",
    "    dpl2 = Dense(1024, activation='relu', kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='dpl2')(dpl1Do)\n",
    "    dplOut = Dense(2, activation='softmax', kernel_initializer='glorot_normal', name='dplOut')(dpl2)\n",
    "    \n",
    "    lplOut._uses_learning_phase = True\n",
    "    \n",
    "    #stitch modle together\n",
    "    vggConvSleep = Model(inputs=vggInput, outputs=[lplOut, dplOut])\n",
    "    \n",
    "    if not vgg_train:\n",
    "        for layer in vggConvSleep.layers[:2]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # Optimizer\n",
    "    optimize = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    #Custom loss func.\n",
    "    def lableLoss(y_true, y_pred):\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        half = 20\n",
    "        in_test_phase(tf.reduce_mean(categorical_crossentropy(y_true[half:,:], y_pred[half:,:])), \n",
    "                      tf.reduce_mean(categorical_crossentropy(y_true[:half,:], y_pred[:half,:])))\n",
    "    \n",
    "    def LPM_loss(y_true, y_pred):\n",
    "        #with K.name_scope('lpmLoss'):\n",
    "            lossLpm = categorical_crossentropy(y_true[0:batch_size//2, :], y_pred[0:batch_size//2, :])\n",
    "            return lossLpm\n",
    "    \n",
    "    # Compile the model\n",
    "    vggConvSleep.compile(loss={'lplOut':LPM_loss,'dplOut':'categorical_crossentropy'},\n",
    "                         optimizer=optimize, metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Get model summary\n",
    "    vggConvSleep.summary()\n",
    "    \n",
    "    return vggConvSleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
