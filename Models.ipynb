{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Input, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy as acc\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.losses import categorical_crossentropy\n",
    "from extras.flip_gradient import flip_gradient\n",
    "from numpy import floor_divide\n",
    "import numpy as np\n",
    "#from ourUtils import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lable modle without DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lable_model(l2_reg = 0.01, do_rate = 0, vgg_train = True, nrUnits = [2048, 1024]):\n",
    "    # Load the convolutional part of the VGG16 network \n",
    "    vgg16Conv = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Input to network\n",
    "    vggInput = Input(shape=(224, 224, 3), name='image_input')\n",
    "    # Output of convolutional part\n",
    "    output_vgg16Conv = vgg16Conv(vggInput)\n",
    "    # Stack lable layers\n",
    "    preDns = Flatten(name='preLp')(output_vgg16Conv)\n",
    "    preDnsBN = BatchNormalization()(preDns)\n",
    "    preDnsDo = Dropout(rate=do_rate, seed=42, name='preDnsDo')(preDnsBN)\n",
    "    #dns1 = Dense(nrUnits[0], activation='relu', kernel_initializer='glorot_normal', \n",
    "    #             bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl1')(preDnsDo)\n",
    "    dns1 = Dense(nrUnits[0], activation=None, kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl1')(preDnsDo)\n",
    "    dns1BN = BatchNormalization()(dns1)\n",
    "    dns1ACT = Activation('relu')(dns1BN)\n",
    "    dns1Do = Dropout(rate=do_rate, seed=42, name='lpl1Do')(dns1ACT)\n",
    "    dns2 = Dense(nrUnits[1], activation=None, kernel_initializer='glorot_normal', \n",
    "                 bias_initializer='glorot_normal', kernel_regularizer=l2(l=l2_reg), name='lpl2')(dns1Do)\n",
    "    dns2BN = BatchNormalization()(dns2)\n",
    "    dns2ACT = Activation('relu')(dns2BN)\n",
    "    modelOut = Dense(5, activation='softmax', kernel_initializer='glorot_normal', name='lplOut')(dns2ACT)\n",
    "\n",
    "    vggConvSleep = Model(inputs=vggInput, outputs=modelOut)\n",
    "\n",
    "    if not vgg_train:\n",
    "        for layer in vggConvSleep.layers[1].layers[:-2]:\n",
    "            layer.trainable = False\n",
    "        vggConvSleep.layers[1].layers[-2].kernel_regularizer = vggConvSleep.layers[-4].kernel_regularizer\n",
    "\n",
    "    # Optimizer\n",
    "    optimize = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    # Compile the model\n",
    "    vggConvSleep.compile(loss='categorical_crossentropy', optimizer=optimize, metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Get model summary\n",
    "    vggConvSleep.summary()\n",
    "    \n",
    "    return vggConvSleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DA_model(lamFunk, l2_reg = 0.01, do_rate_dpl = 0, do_rate_lpl = 0, vgg_train = True, nrUnits = [2048, 1024]):\n",
    "    \n",
    "    # Load the convolutional part of the VGG16 network \n",
    "    vgg16Conv = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Input to network\n",
    "    vggInput = Input(shape=(224, 224, 3), name='image_input')\n",
    "    # Output of convolutional part\n",
    "    output_vggConv = vgg16Conv(vggInput)\n",
    "    # pre Dence layer\n",
    "    preDns = Flatten(name='preDa')(output_vggConv)\n",
    "    # create the shared part as a model instance\n",
    "    sharedVGG16 = Model(inputs=vggInput, outputs=preDns)\n",
    "    \n",
    "    #Create lable predictive model\n",
    "    lpl_input = Input(shape=(224,224,3), name='lplInput')\n",
    "    # run lpl input through the shared part of the network\n",
    "    lpl_vgg_out = sharedVGG16(lpl_input)\n",
    "    lpl_vgg_out_BN = BatchNormalization(name='VGGoutBN')(lpl_vgg_out)\n",
    "    lpl_vgg_outDo = Dropout(rate=do_rate_lpl, seed=42, name='lpl_vgg_outDo')(lpl_vgg_out_BN)\n",
    "    lpl1 = Dense(nrUnits[0], activation=None, kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='lpl1')(lpl_vgg_outDo)\n",
    "    lpl1BN = BatchNormalization(name='lpl1BN')(lpl1)\n",
    "    lpl1ACT = Activation('relu',name='lpl1ACT')(lpl1BN)\n",
    "    lpl1Do = Dropout(rate=do_rate_lpl, seed=42, name='lpl1Do')(lpl1ACT)\n",
    "    lpl2 = Dense(nrUnits[1], activation=None, kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='lpl2')(lpl1Do)\n",
    "    lpl2BN = BatchNormalization(name='lpl2BN')(lpl2)\n",
    "    lpl2ACT = Activation('relu', name='lpl2ACT')(lpl2BN)\n",
    "    lplOut = Dense(5, activation='softmax', kernel_initializer='glorot_normal', name='lplOut')(lpl2ACT)\n",
    "    \n",
    "    #Create domain predictive model \n",
    "    dpl_input = Input(shape=(224,224,3), name='dplInput')\n",
    "    # run dpl input through the shared part of the network\n",
    "    dpl_vgg_out = sharedVGG16(dpl_input)\n",
    "    #lambdalayer for the flip gradient\n",
    "    flipGrad = Lambda(lambda x: flip_gradient(x,lamFunk),name='flipGrad')(dpl_vgg_out)\n",
    "    dpl1 = Dense(nrUnits[0], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='dpl1')(flipGrad)\n",
    "    dpl1Do = Dropout(rate=do_rate_dpl, seed=42, name='dpl1Do')(dpl1)\n",
    "    dpl2 = Dense(nrUnits[1], activation='relu', kernel_initializer='glorot_normal', \n",
    "                 kernel_regularizer=l2(l=l2_reg), name='dpl2')(dpl1Do)\n",
    "    dplOut = Dense(2, activation='softmax', kernel_initializer='glorot_normal', name='dplOut')(dpl2)\n",
    "    \n",
    "    #stitch modle together\n",
    "    DAnetwork = Model(inputs=[lpl_input, dpl_input], outputs=[lplOut, dplOut]) \n",
    "    \n",
    "    if not vgg_train:\n",
    "        for layer in DAnetwork.layers[1].layers[1].layers[:-2]:\n",
    "            layer.trainable = False\n",
    "        DAnetwork.layers[1].layers[1].layers[-2].kernel_regularizer = DAnetwork.layers[-8].kernel_regularizer    \n",
    "    \n",
    "    # Optimizer\n",
    "    optimize = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    # Compile the model\n",
    "    DAnetwork.compile(optimizer=optimize, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Get model summary\n",
    "    DAnetwork.summary()\n",
    "    \n",
    "    return DAnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 25088)        14714688    lplInput[0][0]                   \n",
      "                                                                 dplInput[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "VGGoutBN (BatchNormalization)   (None, 25088)        100352      model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl_vgg_outDo (Dropout)         (None, 25088)        0           VGGoutBN[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lpl1 (Dense)                    (None, 2048)         51382272    lpl_vgg_outDo[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lpl1BN (BatchNormalization)     (None, 2048)         8192        lpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dplInput (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lpl1ACT (Activation)            (None, 2048)         0           lpl1BN[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lpl1Do (Dropout)                (None, 2048)         0           lpl1ACT[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flipGrad (Lambda)               (None, 25088)        0           model_3[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lpl2 (Dense)                    (None, 1024)         2098176     lpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl1 (Dense)                    (None, 2048)         51382272    flipGrad[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lpl2BN (BatchNormalization)     (None, 1024)         4096        lpl2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dpl1Do (Dropout)                (None, 2048)         0           dpl1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lpl2ACT (Activation)            (None, 1024)         0           lpl2BN[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dpl2 (Dense)                    (None, 1024)         2098176     dpl1Do[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lplOut (Dense)                  (None, 5)            5125        lpl2ACT[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dplOut (Dense)                  (None, 2)            2050        dpl2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 121,795,399\n",
      "Trainable params: 121,739,079\n",
      "Non-trainable params: 56,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test = DA_model(0.0, l2_reg = 0.01, do_rate_dpl = 0, do_rate_lpl = 0, vgg_train = True, nrUnits = [2048, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-6e8ec5baa617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test.layers[].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow env",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
